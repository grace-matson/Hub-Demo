{
  "properties": {
    "doc.Confluent-sparksink": "# Confluent Streaming Sink\n\n\nDescription\n-----------\nThis sink writes data to Confluent.\nSends message to specified Kafka topic per received record. It can also be\nconfigured to partition events being written to kafka based on a configurable key. \nThe sink can also be configured to operate in sync or async mode and apply different\ncompression types to events.\nCan be used with self-managed Confluent Platform or Confluent Cloud. Supports Schema Registry.\n\n\nProperties\n----------\n**Reference Name:** This will be used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**Kafka Brokers:** List of Kafka brokers specified in host1:port1,host2:port2 form. (Macro-enabled)\n\n**Kafka Topic:** The Kafka topic to read from. (Macro-enabled)\n\n**Async:** Specifies whether an acknowledgment is required from broker that message was received. Default is No.\n\n**Compression Type:** Compression type to be applied on message.\n\n**Time Field:** Optional name of the field containing the read time of the message. \nIf this is not set, message will be send with current timestamp. \nIf set, this field must be present in the input schema and must be a long.\n\n**Key Field:** Optional name of the field containing the message key.\nIf this is not set, message will be send without a key.\nIf set, this field must be present in the schema property and must be of type bytes.\n\n**Partition Field:** Optional name of the field containing the partition the message should be written to.\nIf this is not set, default partition will be used for all messages.\nIf set, this field must be present in the schema property and must be an int.\n\n**Message Format:** Optional format a structured record should be converted to.\n Required if used without Schema Registry.\n\n**Additional Kafka Producer Properties:** Additional Kafka producer properties to set.\n\n**Cluster API Key:** The Confluent API Key used for the source.\n\n**Cluster API Secret:** The Confluent API Secret used for the source.\n\n**Schema Registry URL:** The Schema Registry endpoint URL.\n\n**Schema Registry API Key:** The Schema Registry API Key.\n\n**Schema Registry API Secret:** The Schema Registry API Secret.\n\nExample\n-------\nThis example writes structured record to kafka topic 'alarm' in asynchronous manner \nusing compression type 'gzip'. The written events will be written in csv format \nto kafka running at localhost. The Kafka partition will be decided based on the provided key 'ts'.\nAdditional properties like number of acknowledgements and client id can also be provided.\n\n```json\n{\n    \"name\": \"Confluent\",\n    \"type\": \"batchsink\",\n    \"properties\": {\n        \"referenceName\": \"Kafka\",\n        \"brokers\": \"host1.example.com:9092,host2.example.com:9092\",\n        \"topic\": \"alarm\",\n        \"async\": \"true\",\n        \"compressionType\": \"gzip\",\n        \"format\": \"CSV\",\n        \"kafkaProperties\": \"acks:2,client.id:myclient\",\n        \"key\": \"message\",\n        \"clusterApiKey\": \"\",\n        \"clusterApiSecret\": \"\"\n    }\n}\n```\n",
    "doc.Confluent-streamingsource": "# Confluent Streaming Source\n\n\nDescription\n-----------\nThis source reads data from Confluent.\nEmits a record per message from specified Kafka topic. \nCan be used with self-managed Confluent Platform or Confluent Cloud. Supports Schema Registry.\n\nCan be configured to parse values from source in following ways:\n1. User-defined format. Use **Message Format** field to choose any format supported by CDAP.\n1. Schema Registry. Requires credentials for Schema Registry to be specified. \nUses Avro schemas to deserialize Kafka messages. Use **Get Schema** button to fetch key and value schemas from registry.\n1. Binary format. Used in case if no message format or Schema Registry credentials were provided.\n\n\nProperties\n----------\n**Reference Name:** This will be used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**Kafka Brokers:** List of Kafka brokers specified in host1:port1,host2:port2 form. (Macro-enabled)\n\n**Kafka Topic:** The Kafka topic to read from. (Macro-enabled)\n\n**Topic Partitions:** List of topic partitions to read from. If not specified, all partitions will be read. (Macro-enabled)\n\n**Default Initial Offset:** The default initial offset for all topic partitions.\nAn offset of -2 means the smallest offset. An offset of -1 means the latest offset. Defaults to -1.\nOffsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read.\nIf you wish to set different initial offsets for different partitions, use the initialPartitionOffsets property. (Macro-enabled)\n\n**Initial Partition Offsets:** The initial offset for each topic partition. If this is not specified,\nall partitions will use the same initial offset, which is determined by the defaultInitialOffset property.\nAny partitions specified in the partitions property, but not in this property will use the defaultInitialOffset.\nAn offset of -2 means the smallest offset. An offset of -1 means the latest offset.\nOffsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. (Macro-enabled)\n\n**Time Field:** Optional name of the field containing the read time of the batch.\nIf this is not set, no time field will be added to output records.\nIf set, this field must be present in the schema property and must be a long.\n\n**Key Field:** Optional name of the field containing the message key.\nIf this is not set, no key field will be added to output records.\nIf set, this field must be present in the schema property and must be bytes.\n\n**Partition Field:** Optional name of the field containing the partition the message was read from.\nIf this is not set, no partition field will be added to output records.\nIf set, this field must be present in the schema property and must be an int.\n\n**Offset Field:** Optional name of the field containing the partition offset the message was read from.\nIf this is not set, no offset field will be added to output records.\nIf set, this field must be present in the schema property and must be a long.\n\n**Message Format:** Optional format of the Kafka event message. Any format supported by CDAP is supported.\nFor example, a value of 'csv' will attempt to parse Kafka payloads as comma-separated values.\nIf no format is given, Kafka message payloads will be treated as bytes.\n\n**Max Rate Per Partition:** Maximum number of records to read per second per partition. Defaults to 1000.\n\n**Additional Kafka Consumer Properties:** Additional Kafka consumer properties to set.\n\n**Cluster API Key:** The Confluent API Key used for the source.\n\n**Cluster API Secret:** The Confluent API Secret used for the source.\n\n**Schema Registry URL:** The Schema Registry endpoint URL.\n\n**Schema Registry API Key:** The Schema Registry API Key.\n\n**Schema Registry API Secret:** The Schema Registry API Secret.\n\n**Value Field:** The name of the field containing the message value. Required to fetch schema from Schema Registry.\n\n**Schema:** Output schema of the source. If you would like the output records to contain a field with the\nKafka message key, the schema must include a field of type bytes/nullable bytes or string/nullable string, and you must \nset the **Key Field** property to that field's name. Similarly, if you would like the output records to contain a field\nwith the timestamp of when the record was read, the schema must include a field of type long or nullable long, and you\nmust set the **Time Field** property to that field's name. Any field that is not the **Time Field** or **Key Field**\nwill be used in conjunction with the format to parse Kafka message payloads. If used with Schema Registry then should\nbe fetched using **Get Schema** button.\n\nExample\n-------\n***Example 1:*** Read from the 'purchases' topic of a Kafka instance running\non brokers host1.example.com:9092 and host2.example.com:9092. The source will add\na time field named 'readTime' that contains a timestamp corresponding to the micro\nbatch when the record was read. It will also contain a field named 'key' which will have\nthe message key in it. It parses the Kafka messages using the 'csv' format\nwith 'user', 'item', 'count', and 'price' as the message schema.\n\n```json\n{\n    \"name\": \"Confluent\",\n    \"type\": \"streamingsource\",\n    \"properties\": {\n        \"topics\": \"purchases\",\n        \"brokers\": \"host1.example.com:9092,host2.example.com:9092\",\n        \"format\": \"csv\",\n        \"timeField\": \"readTime\",\n        \"keyField\": \"key\",\n        \"clusterApiKey\": \"\",\n        \"clusterApiSecret\": \"\",\n        \"defaultInitialOffset\": \"-2\",\n        \"schema\": \"{\n            \\\"type\\\":\\\"record\\\",\n            \\\"name\\\":\\\"purchase\\\",\n            \\\"fields\\\":[\n                {\\\"name\\\":\\\"readTime\\\",\\\"type\\\":\\\"long\\\"},\n                {\\\"name\\\":\\\"key\\\",\\\"type\\\":\\\"bytes\\\"},\n                {\\\"name\\\":\\\"user\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"item\\\",\\\"type\\\":\\\"string\\\"},\n                {\\\"name\\\":\\\"count\\\",\\\"type\\\":\\\"int\\\"},\n                {\\\"name\\\":\\\"price\\\",\\\"type\\\":\\\"double\\\"}\n            ]\n        }\"\n    }\n}\n```\n\nFor each Kafka message read, it will output a record with the schema:\n\n| field name  | type             |\n| ----------- | ---------------- |\n| readTime    | long             |\n| key         | bytes            |\n| user        | string           |\n| item        | string           |\n| count       | int              |\n| price       | double           |\n\nNote that the readTime field is not derived from the Kafka message, but from the time that the\nmessage was read.\n",
    "widgets.Confluent-streamingsource": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"widget-attributes\": {\n      \"default-schema\": {\n        \"name\": \"etlSchemaBody\",\n        \"type\": \"record\",\n        \"fields\": [{\n          \"name\": \"message\",\n          \"type\": \"string\"\n        }]\n      },\n      \"schema-default-type\": \"string\",\n      \"property-watch\": \"format\"\n    }\n  }],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"referenceName\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Kafka Configuration\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"referenceName\",\n          \"label\": \"Reference Name\"\n        },\n        {\n          \"widget-type\": \"csv\",\n          \"name\": \"brokers\",\n          \"label\": \"Kafka Brokers\",\n          \"widget-attributes\": {\"delimiter\": \",\"}\n        },\n        {\n          \"widget-type\": \"connection-browser\",\n          \"widget-category\": \"plugin\",\n          \"widget-attributes\": {\n            \"label\": \"Browse\",\n            \"connectionType\": \"KAFKA\"\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"topic\",\n          \"label\": \"Kafka Topic\"\n        },\n        {\n          \"widget-type\": \"csv\",\n          \"name\": \"partitions\",\n          \"label\": \"Topic Partitions\",\n          \"widget-attributes\": {\"delimiter\": \",\"}\n        },\n        {\n          \"widget-type\": \"number\",\n          \"name\": \"defaultInitialOffset\",\n          \"label\": \"Default Initial Offset\",\n          \"widget-attributes\": {\"default\": -1}\n        },\n        {\n          \"widget-type\": \"keyvalue\",\n          \"name\": \"initialPartitionOffsets\",\n          \"label\": \"Initial Partition Offsets\",\n          \"widget-attributes\": {\n            \"key-placeholder\": \"Partition\",\n            \"value-placeholder\": \"Offset\",\n            \"showDelimiter\": \"false\"\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"timeField\",\n          \"label\": \"Time Field\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"keyField\",\n          \"label\": \"Key Field\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"partitionField\",\n          \"label\": \"Partition Field\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"offsetField\",\n          \"label\": \"Offset Field\"\n        },\n        {\n          \"widget-type\": \"number\",\n          \"name\": \"maxRatePerPartition\",\n          \"label\": \"Max Rate Per Partition\",\n          \"widget-attributes\": {\"default\": 1000}\n        },\n        {\n          \"widget-type\": \"keyvalue\",\n          \"name\": \"kafkaProperties\",\n          \"label\": \"Additional Kafka Consumer Properties\",\n          \"widget-attributes\": {\n            \"key-placeholder\": \"Kafka consumer property\",\n            \"value-placeholder\": \"Kafka consumer property value\",\n            \"showDelimiter\": \"false\"\n          }\n        }\n      ]\n    },\n    {\n      \"label\": \"Authentication\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"clusterApiKey\",\n          \"label\": \"Cluster API Key\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"clusterApiSecret\",\n          \"label\": \"Cluster API Secret\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Schema Registry\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"schemaRegistryUrl\",\n          \"label\": \"Schema Registry URL\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"schemaRegistryApiKey\",\n          \"label\": \"Schema Registry API Key\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"schemaRegistryApiSecret\",\n          \"label\": \"Schema Registry API Secret\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"valueField\",\n          \"label\": \"Value Field\",\n          \"plugin-function\": {\n            \"widget\": \"outputSchema\",\n            \"label\": \"Get Schema\",\n            \"position\": \"bottom\",\n            \"multiple-inputs\": false,\n            \"button-class\": \"btn-hydrator\"\n          }\n        }\n      ]\n    },\n    {\n      \"label\": \"Message Configuration\",\n      \"properties\": [{\n        \"widget-type\": \"select\",\n        \"name\": \"format\",\n        \"label\": \"Message Format\",\n        \"widget-attributes\": {\n          \"default\": \"\",\n          \"values\": [\n            \"\",\n            \"avro\",\n            \"binary\",\n            \"clf\",\n            \"csv\",\n            \"grok\",\n            \"syslog\",\n            \"text\",\n            \"tsv\"\n          ]\n        }\n      }]\n    }\n  ],\n  \"display-name\": \"Confluent Kafka\",\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAABtUlEQVRoQ2P8////f4ZhABhHPTLI\\r\\nYnE0RgZZhDCMxgiuGLn68itDxoYbREdYtIEEQ4a5NIp625lnMfSri3IxzAnSxGku1WMEmyMI+epw\\r\\nujFcief8Cwxffv3FqgVZHbqCQeeR5n33GXbdfjfwHgG54Pyzz4QiAS5vKMWLofbF518Mzz//RBHH\\r\\npg5ZAdVjhGgfUFnhqEeoHKAUGzcaIxQHIZUNICpG1px8idXaEHNxuPiD198Zztz7RJLzkPWTpBGL\\r\\nYqp5BJdn8TmQ7h7Zf+0dw9vPvzHchO4QUjyjIsHFYCCPWYeQGzNExQi5htNT36hH6BnaxNg1GiPE\\r\\nhBI91eCNkb99wShuYS5ai+G2f5OjGP7/Rm2pYvMAIys7A1PuMpr5DadH/mZIYbWUecYzuDguNfhc\\r\\ni6yfmr4a/h4BhdawSFrUjHZ6mDVa/NIjlEmxYzRGSAkteqglKkaGTTOemB4iKX0RWAzRvWNFjEeG\\r\\nRFeXHmmcUjuIyiOUWkIP/aMeoUcok2LHaIzgC61hMa1A6YzVoJnoodQjg2bqbdhMhpJS0lBT7Wip\\r\\nRc3QpIZZozFCjVCkphnDJkYAbxpdepJ9dHIAAAAASUVORK5C\"},\n    \"type\": \"inline\"\n  }\n}",
    "widgets.Confluent-sparksink": "{\n  \"outputs\": [],\n  \"jump-config\": {\"datasets\": [{\"ref-property-name\": \"referenceName\"}]},\n  \"metadata\": {\"spec-version\": \"1.5\"},\n  \"configuration-groups\": [\n    {\n      \"label\": \"Kafka Configuration\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"referenceName\",\n          \"label\": \"Reference Name\"\n        },\n        {\n          \"widget-type\": \"csv\",\n          \"name\": \"brokers\",\n          \"label\": \"Kafka Brokers\",\n          \"widget-attributes\": {\"delimiter\": \",\"}\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"topic\",\n          \"label\": \"Kafka Topic\"\n        },\n        {\n          \"widget-type\": \"toggle\",\n          \"name\": \"async\",\n          \"label\": \"Async\",\n          \"widget-attributes\": {\n            \"default\": \"false\",\n            \"off\": {\n              \"label\": \"No\",\n              \"value\": \"false\"\n            },\n            \"on\": {\n              \"label\": \"Yes\",\n              \"value\": \"true\"\n            }\n          }\n        },\n        {\n          \"widget-type\": \"select\",\n          \"name\": \"compressionType\",\n          \"label\": \"Compression Type\",\n          \"widget-attributes\": {\n            \"default\": \"none\",\n            \"values\": [\n              \"none\",\n              \"gzip\",\n              \"snappy\"\n            ]\n          }\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"timeField\",\n          \"label\": \"Time Field\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"keyField\",\n          \"label\": \"Key Field\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"partitionField\",\n          \"label\": \"Partition Field\"\n        },\n        {\n          \"widget-type\": \"keyvalue\",\n          \"name\": \"kafkaProperties\",\n          \"label\": \"Additional Kafka Producer Properties\",\n          \"widget-attributes\": {\n            \"key-placeholder\": \"Kafka producer property\",\n            \"value-placeholder\": \"Kafka producer property value\",\n            \"showDelimiter\": \"false\"\n          }\n        }\n      ]\n    },\n    {\n      \"label\": \"Authentication\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"clusterApiKey\",\n          \"label\": \"Cluster API Key\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"clusterApiSecret\",\n          \"label\": \"Cluster API Secret\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Schema Registry\",\n      \"properties\": [\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"schemaRegistryUrl\",\n          \"label\": \"Schema Registry URL\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"schemaRegistryApiKey\",\n          \"label\": \"Schema Registry API Key\"\n        },\n        {\n          \"widget-type\": \"textbox\",\n          \"name\": \"schemaRegistryApiSecret\",\n          \"label\": \"Schema Registry API Secret\"\n        }\n      ]\n    },\n    {\n      \"label\": \"Message Configuration\",\n      \"properties\": [{\n        \"widget-type\": \"select\",\n        \"name\": \"format\",\n        \"label\": \"Message Format\",\n        \"widget-attributes\": {\n          \"default\": \"\",\n          \"values\": [\n            \"\",\n            \"CSV\",\n            \"JSON\"\n          ]\n        }\n      }]\n    }\n  ],\n  \"display-name\": \"Confluent Kafka\",\n  \"icon\": {\n    \"arguments\": {\"data\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAABtUlEQVRoQ2P8////f4ZhABhHPTLI\\r\\nYnE0RgZZhDCMxgiuGLn68itDxoYbREdYtIEEQ4a5NIp625lnMfSri3IxzAnSxGku1WMEmyMI+epw\\r\\nujFcief8Cwxffv3FqgVZHbqCQeeR5n33GXbdfjfwHgG54Pyzz4QiAS5vKMWLofbF518Mzz//RBHH\\r\\npg5ZAdVjhGgfUFnhqEeoHKAUGzcaIxQHIZUNICpG1px8idXaEHNxuPiD198Zztz7RJLzkPWTpBGL\\r\\nYqp5BJdn8TmQ7h7Zf+0dw9vPvzHchO4QUjyjIsHFYCCPWYeQGzNExQi5htNT36hH6BnaxNg1GiPE\\r\\nhBI91eCNkb99wShuYS5ai+G2f5OjGP7/Rm2pYvMAIys7A1PuMpr5DadH/mZIYbWUecYzuDguNfhc\\r\\ni6yfmr4a/h4BhdawSFrUjHZ6mDVa/NIjlEmxYzRGSAkteqglKkaGTTOemB4iKX0RWAzRvWNFjEeG\\r\\nRFeXHmmcUjuIyiOUWkIP/aMeoUcok2LHaIzgC61hMa1A6YzVoJnoodQjg2bqbdhMhpJS0lBT7Wip\\r\\nRc3QpIZZozFCjVCkphnDJkYAbxpdepJ9dHIAAAAASUVORK5C\"},\n    \"type\": \"inline\"\n  }\n}"
  },
  "parents": [
    "system:cdap-data-pipeline[6.1.2,7.0.0-SNAPSHOT)",
    "system:cdap-data-streams[6.1.2,7.0.0-SNAPSHOT)"
  ]
}