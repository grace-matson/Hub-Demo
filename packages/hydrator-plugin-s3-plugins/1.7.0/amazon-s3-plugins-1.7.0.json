{
  "parents": [
    "system:cdap-data-pipeline[4.1.0,4.3.0-SNAPSHOT)",
    "system:cdap-data-streams[4.1.0,4.3.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.S3-batchsource": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"S3 Batch Source\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"select\",\"label\":\"Authentication Method\",\"name\":\"authenticationMethod\",\"widget-attributes\":{\"values\":[\"Access Credentials\",\"IAM\"],\"default\":\"Access Credentials\"}},{\"widget-type\":\"textbox\",\"label\":\"Access ID\",\"name\":\"accessID\"},{\"widget-type\":\"textbox\",\"label\":\"Access Key\",\"name\":\"accessKey\"},{\"widget-type\":\"textbox\",\"label\":\"Path\",\"name\":\"path\"},{\"widget-type\":\"textbox\",\"label\":\"Regex Path Filter\",\"name\":\"fileRegex\"},{\"widget-type\":\"textbox\",\"label\":\"Maximum Split Size\",\"name\":\"maxSplitSize\"},{\"widget-type\":\"select\",\"label\":\"Read files recursively\",\"name\":\"recursive\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"label\":\"Output Schema Properties\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Path Field\",\"name\":\"pathField\",\"plugin-function\":{\"method\":\"POST\",\"widget\":\"outputSchema\",\"output-property\":\"schema\",\"plugin-method\":\"getSchema\"}},{\"widget-type\":\"select\",\"label\":\"Use File Name as Path Field\",\"name\":\"filenameOnly\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}}]},{\"label\":\"Advanced Properties\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Input Format Class\",\"name\":\"inputFormatClass\"},{\"widget-type\":\"json-editor\",\"label\":\"File System Properties\",\"name\":\"fileSystemProperties\"},{\"widget-type\":\"select\",\"label\":\"Ignore Non-Existing Folders\",\"name\":\"ignoreNonExistingFolders\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"textbox\",\"label\":\"Time Table\",\"name\":\"timeTable\"}]}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"default-schema\":{\"name\":\"fileRecord\",\"type\":\"record\",\"fields\":[{\"name\":\"offset\",\"type\":\"long\"},{\"name\":\"body\",\"type\":\"string\"}]}}}]}",
    "widgets.S3Avro-batchsink": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"S3 Avro Batch Sink\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"select\",\"label\":\"Authentication Method\",\"name\":\"authenticationMethod\",\"widget-attributes\":{\"values\":[\"Access Credentials\",\"IAM\"],\"default\":\"Access Credentials\"}},{\"widget-type\":\"textbox\",\"label\":\"Access ID\",\"name\":\"accessID\"},{\"widget-type\":\"textbox\",\"label\":\"Access Key\",\"name\":\"accessKey\"},{\"widget-type\":\"textbox\",\"label\":\"Base Path\",\"name\":\"basePath\"},{\"widget-type\":\"textbox\",\"label\":\"Path Format\",\"name\":\"pathFormat\"},{\"widget-type\":\"select\",\"label\":\"Server Side Encryption\",\"name\":\"enableEncryption\",\"widget-attributes\":{\"values\":[\"True\",\"False\"],\"default\":\"True\"}},{\"widget-type\":\"select\",\"label\":\"Compression Codec\",\"name\":\"compressionCodec\",\"widget-attributes\":{\"values\":[\"None\",\"Snappy\",\"Deflate\"],\"default\":\"None\"}},{\"widget-type\":\"json-editor\",\"label\":\"File System Properties\",\"name\":\"fileSystemProperties\"}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"schema-types\":[\"boolean\",\"int\",\"long\",\"float\",\"double\",\"bytes\",\"string\",\"map<string, string>\"],\"schema-default-type\":\"string\"}}]}",
    "widgets.S3Parquet-batchsink": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"S3 Parquet Batch Sink\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"select\",\"label\":\"Authentication Method\",\"name\":\"authenticationMethod\",\"widget-attributes\":{\"values\":[\"Access Credentials\",\"IAM\"],\"default\":\"Access Credentials\"}},{\"widget-type\":\"textbox\",\"label\":\"Access ID\",\"name\":\"accessID\"},{\"widget-type\":\"textbox\",\"label\":\"Access Key\",\"name\":\"accessKey\"},{\"widget-type\":\"textbox\",\"label\":\"Base Path\",\"name\":\"basePath\"},{\"widget-type\":\"textbox\",\"label\":\"Path Format\",\"name\":\"pathFormat\"},{\"widget-type\":\"select\",\"label\":\"Server Side Encryption\",\"name\":\"enableEncryption\",\"widget-attributes\":{\"values\":[\"True\",\"False\"],\"default\":\"True\"}},{\"widget-type\":\"select\",\"label\":\"Compression Codec\",\"name\":\"compressionCodec\",\"widget-attributes\":{\"values\":[\"None\",\"Snappy\",\"GZip\",\"LZO\"],\"default\":\"None\"}},{\"widget-type\":\"json-editor\",\"label\":\"File System Properties\",\"name\":\"fileSystemProperties\"}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"schema-types\":[\"boolean\",\"int\",\"long\",\"float\",\"double\",\"bytes\",\"string\",\"map<string, string>\"],\"schema-default-type\":\"string\"}}]}",
    "doc.S3-batchsource": "# Amazon S3 Batch Source\n\n\nDescription\n-----------\nBatch source to use Amazon S3 as a Source.\n\n\nUse Case\n--------\nThis source is used whenever you need to read from Amazon S3.\nFor example, you may want to read in log files from S3 every hour and then store\nthe logs in a TimePartitionedFileSet.\n\n\nProperties\n----------\n**referenceName:** This will be used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**authenticationMethod:** Authentication method to access S3. Defaults to Access Credentials.\n User need to have AWS environment only to use IAM role based authentication. URI scheme should be s3a:// for S3AFileSystem or s3n:// for S3NativeFileSystem. (Macro-enabled)\n\n**accessID:** Access ID of the Amazon S3 instance to connect to. Mandatory if authentication method is Access credentials. (Macro-enabled)\n\n**accessKey:** Access Key of the Amazon S3 instance to connect to. Mandatory if authentication method is Access credentials. (Macro-enabled)\n\n**path:** Path to file(s) to be read. If a directory is specified,\nterminate the path name with a '/'. The path uses filename expansion (globbing) to read files. (Macro-enabled)\n\n**fileRegex:** Regex to filter out files in the path. It accepts regular expression which is applied to the complete\npath and returns the list of files that match the specified pattern.\nTo use the *TimeFilter*, input ``timefilter``. The TimeFilter assumes that it is\nreading in files with the File log naming convention of *YYYY-MM-DD-HH-mm-SS-Tag*.\nThe TimeFilter reads in files from the previous hour if the field ``timeTable`` is\nleft blank. If it is currently *2015-06-16-15* (June 16th 2015, 3pm), it will read\nin files that contain *2015-06-16-14* in the filename. If the field ``timeTable`` is\npresent, then it will read in files that have not yet been read. (Macro-enabled)\n\n**timeTable:** Name of the Table that keeps track of the last time files\nwere read in. (Macro-enabled)\n\n**inputFormatClass:** Name of the input format class, which must be a\nsubclass of FileInputFormat. Defaults to TextInputFormat. (Macro-enabled)\n\n**maxSplitSize:** Maximum split-size for each mapper in the MapReduce Job. Defaults to 128MB. (Macro-enabled)\n\n**ignoreNonExistingFolders:** Identify if path needs to be ignored or not, for case when directory or file does not\nexists. If set to true it will treat the not present folder as 0 input and log a warning. Default is false.\n\n**recursive:** Boolean value to determine if files are to be read recursively from the path. Default is false.\n\n\nExample\n-------\nThis example connects to Amazon S3 using Access Credentials and reads in files found in the specified directory while\nusing the stateful ``timefilter``, which ensures that each file is read only once. The ``timefilter``\nrequires that files be named with either the convention \"yy-MM-dd-HH...\" (S3) or \"...'.'yy-MM-dd-HH...\"\n(Cloudfront). The stateful metadata is stored in a table named 'timeTable'. With the maxSplitSize\nset to 1MB, if the total size of the files being read is larger than 1MB, CDAP will\nconfigure Hadoop to use one mapper per MB:\n\n    {\n        \"name\": \"S3\",\n        \"type\": \"batchsource\",\n        \"properties\": {\n            \"authenticationMethod\": \"Access Credentials\",\n            \"accessKey\": \"key\",\n            \"accessID\": \"ID\",\n            \"path\": \"s3a://path/to/logs/\",\n            \"fileRegex\": \"timefilter\",\n            \"timeTable\": \"timeTable\",\n            \"maxSplitSize\": \"1048576\",\n            \"ignoreNonExistingFolders\": \"false\",\n            \"recursive\": \"false\"\n        }\n    }\n",
    "doc.S3Avro-batchsink": "# Amazon S3 Avro Batch Sink\n\n\nDescription\n-----------\nA batch sink for writing to Amazon S3 in Avro format.\n\n\nUse Case\n--------\nThis source is used whenever you need to write to Amazon S3 in Avro format. For example,\nyou might want to create daily snapshots of a database by reading the entire contents of a\ntable, writing to this sink, and then other programs can analyze the contents of the\nspecified file. The output of the run will be stored in a directory with suffix\n'yyyy-MM-dd-HH-mm' from the base path provided.\n\n\nProperties\n----------\n**referenceName:** This will be used to uniquely identify this sink for lineage, annotating metadata, etc.\n\n**authenticationMethod:** Authentication method to access S3. Defaults to Access Credentials.\n User need to have AWS environment only to use IAM role based authentication. \n  URI scheme should be s3a:// for S3AFileSystem or s3n:// for S3NativeFileSystem. (Macro-enabled)\n\n**accessID:** Access ID of the Amazon S3 instance to connect to. (Macro-enabled)\n\n**accessKey:** Access Key of the Amazon S3 instance to connect to. (Macro-enabled)\n\n**basePath:** The S3 path where the data is stored. Example: 's3a://logs'. (Macro-enabled)\n\n**enableEncryption:** Server side encryption. Defaults to True. Sole supported algorithm is AES256. (Macro-enabled)\n\n**fileSystemProperties:** A JSON string representing a map of properties needed for the\ndistributed file system. The property names needed for S3 (*accessID* and *accessKey*)\nwill be included as ``'fs.s3a.access.key'`` and ``'fs.s3a.secret.key'`` for S3AFileSystem. \nFor S3NativeFileSystem ``'fs.s3n.awsSecretAccessKey'`` and ``'fs.s3n.awsAccessKeyId'`` will be used. (Macro-enabled)\n\n**pathFormat:** The format for the path that will be suffixed to the basePath; for\nexample: the format ``'yyyy-MM-dd-HH-mm'`` will create a file path ending in\n``'2015-01-01-20-42'``. Default format used is ``'yyyy-MM-dd-HH-mm'``. (Macro-enabled)\n\n**schema:** The Avro schema of the record being written to the sink as a JSON object. (Macro-enabled)\n\n**compressionCodec:** Optional parameter to determine the compression codec to use on the resulting data. \nValid values are None, Snappy, and Deflate.\n\n\nExample\n-------\nThis example will use Access Credentials authentication and write to an S3 output located at ``s3a://logs``. It will write data in\nAvro format compressed using Snappy format and using the given schema. Every time the pipeline \nruns, a new output directory from the base path (``s3a://logs``) will be created which \nwill have the directory name corresponding to the start time in ``yyyy-MM-dd-HH-mm`` format:\n\n    {\n        \"name\": \"S3Avro\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"authenticationMethod\": \"Access Credentials\",\n            \"accessKey\": \"key\",\n            \"accessID\": \"ID\",\n            \"basePath\": \"s3a://logs\",\n            \"pathFormat\": \"yyyy-MM-dd-HH-mm\",\n            \"compressionCodec\": \"Snappy\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n                ]\n            }\"\n        }\n    }",
    "doc.S3Parquet-batchsink": "# Amazon S3 Parquet Batch Sink\n\n\nDescription\n-----------\nA batch sink to write to S3 in Parquet format.\n\n\nUse Case\n--------\nThis source is used whenever you need to write to Amazon S3 in Parquet format. For example,\nyou might want to create daily snapshots of a database by reading the entire contents of a\ntable, writing to this sink, and then other programs can analyze the contents of the\nspecified file. The output of the run will be stored in a directory with suffix\n'yyyy-MM-dd-HH-mm' from the base path provided.\n\n\nProperties\n----------\n**referenceName:** This will be used to uniquely identify this sink for lineage, annotating metadata, etc.\n\n**authenticationMethod:** Authentication method to access S3. Defaults to Access Credentials.\n User need to have AWS environment only to use IAM role based authentication. \n URI scheme should be s3a:// for S3AFileSystem or s3n:// for S3NativeFileSystem. (Macro-enabled)\n\n**accessID:** Access ID of the Amazon S3 instance to connect to. (Macro-enabled)\n\n**accessKey:** Access Key of the Amazon S3 instance to connect to. (Macro-enabled)\n\n**basePath:** The S3 path where the data is stored. Example: 's3a://logs' or 's3n://logs'. (Macro-enabled)\n\n**enableEncryption:** Server side encryption. Defaults to True. Sole supported algorithm is AES256. (Macro-enabled)\n\n**fileSystemProperties:** A JSON string representing a map of properties needed for the\ndistributed file system. The property names needed for S3 (*accessID* and *accessKey*)\nwill be included as ``'fs.s3a.access.key'`` and ``'fs.s3a.secret.key'`` for S3AFileSystem.\nFor S3NativeFileSystem ``'fs.s3n.awsSecretAccessKey'`` and ``'fs.s3n.awsAccessKeyId'`` will be used. (Macro-enabled)\n\n**pathFormat:** The format for the path that will be suffixed to the basePath; for\nexample: the format ``'yyyy-MM-dd-HH-mm'`` will create a file path ending in\n``'2015-01-01-20-42'``. Default format used is ``'yyyy-MM-dd-HH-mm'``. (Macro-enabled)\n\n**schema:** The Parquet schema of the record being written to the sink as a JSON object. (Macro-enabled)\n\n**compressionCodec:** Optional parameter to determine the compression codec to use on the resulting data. \nValid values are None, Snappy, GZip, and LZO.\n\n\nExample\n-------\nThis example will write to an S3 output located at ``s3a://logs``. It will write data in\nParquet format using the given schema. Every time the pipeline runs, a new output directory\nfrom the base path (``s3a://logs``) will be created which will have the directory name\ncorresponding to the start time in ``yyyy-MM-dd-HH-mm`` format:\n\n    {\n        \"name\": \"S3Parquet\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"accessKey\": \"key\",\n            \"accessID\": \"ID\",\n            \"basePath\": \"s3a://logs\",\n            \"pathFormat\": \"yyyy-MM-dd-HH-mm\",\n            \"compressionCodec\": \"Snappy\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"user\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"birthyear\\\",\\\"type\\\":\\\"int\\\"}\n                ]\n            }\"\n        }\n    }"
  }
}