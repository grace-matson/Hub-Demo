{
  "parents": [
    "system:cdap-data-streams[4.2.0-SNAPSHOT,10.0.0-SNAPSHOT)",
    "system:cdap-data-pipeline[4.2.0-SNAPSHOT,10.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.Kafka-batchsink": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"Kafka Producer and Topic Config\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"csv\",\"label\":\"Kafka Brokers\",\"name\":\"brokers\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"textbox\",\"label\":\"Kafka Topic\",\"name\":\"topic\"},{\"widget-type\":\"select\",\"label\":\"Is Async ?\",\"name\":\"async\",\"widget-attributes\":{\"values\":[\"TRUE\",\"FALSE\"],\"default\":\"FALSE\"}},{\"widget-type\":\"select\",\"label\":\"Compression type\",\"name\":\"compressionType\",\"widget-attributes\":{\"values\":[\"none\",\"gzip\",\"snappy\"],\"default\":\"none\"}},{\"widget-type\":\"keyvalue\",\"label\":\"Additional Kafka Producer Properties\",\"name\":\"kafkaProperties\",\"widget-attributes\":{\"showDelimiter\":\"false\",\"key-placeholder\":\"Kafka producer property\",\"value-placeholder\":\"Kafka producer property value\"}}]},{\"label\":\"Message Configuration\",\"properties\":[{\"widget-type\":\"select\",\"label\":\"Message Format\",\"name\":\"format\",\"widget-attributes\":{\"values\":[\"CSV\",\"JSON\"],\"default\":\"CSV\"}},{\"widget-type\":\"textbox\",\"label\":\"Message Key field\",\"name\":\"key\"}]}],\"outputs\":[]}",
    "widgets.Kafka-batchsource": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"Kafka Configuration\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"csv\",\"label\":\"Kafka Brokers\",\"name\":\"kafkaBrokers\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"textbox\",\"label\":\"Kafka Topic\",\"name\":\"topic\"},{\"widget-type\":\"textbox\",\"label\":\"Offset Table Name\",\"name\":\"tableName\"},{\"widget-type\":\"csv\",\"label\":\"Topic Partitions\",\"name\":\"partitions\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"keyvalue\",\"label\":\"Initial Offsets\",\"name\":\"initialPartitionOffsets\",\"widget-attributes\":{\"showDelimiter\":\"false\",\"key-placeholder\":\"Partition\",\"value-placeholder\":\"Offset\"}},{\"widget-type\":\"textbox\",\"label\":\"Key Field\",\"name\":\"keyField\"},{\"widget-type\":\"textbox\",\"label\":\"Partition Field\",\"name\":\"partitionField\"},{\"widget-type\":\"textbox\",\"label\":\"Offset Field\",\"name\":\"offsetField\"}]},{\"label\":\"Format\",\"properties\":[{\"widget-type\":\"select\",\"label\":\"Format\",\"name\":\"format\",\"widget-attributes\":{\"values\":[\"\",\"avro\",\"binary\",\"clf\",\"csv\",\"grok\",\"syslog\",\"text\",\"tsv\"],\"default\":\"\"}}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"default-schema\":{\"name\":\"etlSchemaBody\",\"type\":\"record\",\"fields\":[{\"name\":\"message\",\"type\":[\"bytes\",\"null\"]}]},\"schema-default-type\":\"string\",\"property-watch\":\"format\"}}]}",
    "widgets.Kafka-streamingsource": "{\"metadata\":{\"spec-version\":\"1.3\"},\"configuration-groups\":[{\"label\":\"Kafka Configuration\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"csv\",\"label\":\"Kafka Brokers\",\"name\":\"brokers\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"textbox\",\"label\":\"Kafka Topic\",\"name\":\"topic\"},{\"widget-type\":\"csv\",\"label\":\"Topic Partitions\",\"name\":\"partitions\",\"widget-attributes\":{\"delimiter\":\",\"}},{\"widget-type\":\"textbox\",\"label\":\"Default Initial Offset\",\"name\":\"defaultInitialOffset\"},{\"widget-type\":\"keyvalue\",\"label\":\"Initial Partition Offsets\",\"name\":\"initialPartitionOffsets\",\"widget-attributes\":{\"showDelimiter\":\"false\",\"key-placeholder\":\"Partition\",\"value-placeholder\":\"Offset\"}},{\"widget-type\":\"textbox\",\"label\":\"Time Field\",\"name\":\"timeField\"},{\"widget-type\":\"textbox\",\"label\":\"Key Field\",\"name\":\"keyField\"},{\"widget-type\":\"textbox\",\"label\":\"Partition Field\",\"name\":\"partitionField\"},{\"widget-type\":\"textbox\",\"label\":\"Offset Field\",\"name\":\"offsetField\"},{\"widget-type\":\"textbox\",\"label\":\"Max Rate Per Partition\",\"name\":\"maxRatePerPartition\",\"widget-attributes\":{\"default\":\"1000\"}}]},{\"label\":\"Format\",\"properties\":[{\"widget-type\":\"select\",\"label\":\"Format\",\"name\":\"format\",\"widget-attributes\":{\"values\":[\"\",\"avro\",\"binary\",\"clf\",\"csv\",\"grok\",\"syslog\",\"text\",\"tsv\"],\"default\":\"\"}}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"default-schema\":{\"name\":\"etlSchemaBody\",\"type\":\"record\",\"fields\":[{\"name\":\"message\",\"type\":[\"bytes\",\"null\"]}]},\"schema-default-type\":\"string\",\"property-watch\":\"format\"}}]}",
    "doc.Kafka-batchsink": "# Kafka Sink\n\n\nDescription\n-----------\nKafka sink that allows you to write events into CSV or JSON to kafka.\nPlugin has the capability to push the data to a Kafka topic. It can also be\nconfigured to partition events being written to kafka based on a configurable key. \nThe sink can also be configured to operate in sync or async mode and apply different\ncompression types to events. Kafka sink is compatible with Kafka 0.8, 0.9 and 0.10\n\n\nConfiguration\n-------------\n**referenceName:** This will be used to uniquely identify this sink for lineage, annotating metadata, etc.\n\n**brokers:** List of Kafka brokers specified in host1:port1,host2:port2 form.\n\n**topic:** The Kafka topic to write to.\n\n**async:** Specifies whether writing the events to broker is *Asynchronous* or *Synchronous*.\n\n**compressionType** Compression type to be applied on message. It can be none, gzip or snappy. Default value is none\n\n**format:** Specifies the format of the event published to Kafka. It can be csv or json. Defualt value is csv.\n\n**kafkaProperties** Specifies additional kafka producer properties like acks, client.id as key and value pair.\n\n**key:** Specifies the input field that should be used as the key for the event published into Kafka. \nIt will use String partitioner to determine kafka event should go to which partition. Key field should be of type string.\n\nExample\n-------\nThis example writes structured record to kafka topic 'alarm' in asynchronous manner \nusing compression type 'gzip'. The written events will be written in csv format \nto kafka running at localhost. The Kafka partition will be decided based on the provided key 'ts'.\nAdditional properties like number of acknowledgements and client id can also be provided.\n\n\n    {\n        \"name\": \"Kafka\",\n        \"type\": \"batchsink\",\n        \"properties\": {\n            \"referenceName\": \"Kafka\",\n            \"brokers\": \"localhost:9092\",\n            \"topic\": \"alarm\",\n            \"async\": \"FALSE\",\n            \"compressionType\": \"gzip\",\n            \"format\": \"CSV\",\n            \"kafkaProperties\": \"acks:2,client.id:myclient\",\n            \"key\": \"message\"\n        }\n    }",
    "doc.Kafka-batchsource": "# Kafka Batch Source\n\n\nDescription\n-----------\nKafka batch source. Emits the record from kafka. It will emit a record based on the schema and format \nyou use, or if no schema or format is specified, the message payload will be emitted. The source will \nremember the offset it read last run and continue from that offset for the next run.\n\nUse Case\n--------\nThis source is used whenever you want to read from Kafka. For example, you may want to read messages\nfrom Kafka and write them to a Table.\n\n\nProperties\n----------\n**referenceName:** This will be used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**kafkaBrokers:** List of Kafka brokers specified in host1:port1,host2:port2 form. (Macro-enabled)\n\n**topic:** The Kafka topic to read from. (Macro-enabled)\n\n**tableName:** Optional table name to track the latest offsets read from Kafka. Stores offsets using a composite key \nof reference name, topic name and partition. Only change this table name if you would like to ignore the offsets read \nfrom a Kafka topic by a previous run using the Kafka Batch Source. (Macro-enabled)\n\n**partitions:** List of topic partitions to read from. If not specified, all partitions will be read. (Macro-enabled)\n\n**initialPartitionOffsets:** The initial offset for each topic partition. This offset will only be used for the \nfirst run of the pipeline. Any subsequent run will read from the latest offset from previous run. \nOffsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. (Macro-enabled)\n\n**schema:** Output schema of the source. If you would like the output records to contain a field with the\nKafka message key, the schema must include a field of type bytes or nullable bytes, and you must set the\nkeyField property to that field's name. Similarly, if you would like the output records to contain a field with\nthe timestamp of when the record was read, the schema must include a field of type long or nullable long, and you\nmust set the timeField property to that field's name. Any field that is not the keyField, partitionField and keyField\n will be used in conjuction with the format to parse Kafka message payloads.\n\n**format:** Optional format of the Kafka event message. Any format supported by CDAP is supported.\nFor example, a value of 'csv' will attempt to parse Kafka payloads as comma-separated values.\nIf no format is given, Kafka message payloads will be treated as bytes.\n\n**keyField:** Optional name of the field containing the message key.\nIf this is not set, no key field will be added to output records.\nIf set, this field must be present in the schema property and must be bytes.\n\n**partitionField:** Optional name of the field containing the partition the message was read from.\nIf this is not set, no partition field will be added to output records.\nIf set, this field must be present in the schema property and must be an int.\n\n**offsetField:** Optional name of the field containing the partition offset the message was read from.\nIf this is not set, no offset field will be added to output records.\nIf set, this field must be present in the schema property and must be a long.\n\n\nExample\n-------\nThis example reads from the 'purchases' topic of a Kafka instance running\non brokers host1.example.com:9092 and host2.example.com:9092. The source will add\na field named 'key' which will have the message key in it. It parses the Kafka messages \nusing the 'csv' format with 'user', 'item', 'count', and 'price' as the message schema.\n\n    {\n        \"name\": \"Kafka\",\n        \"type\": \"streamingsource\",\n        \"properties\": {\n            \"topics\": \"purchases\",\n            \"brokers\": \"host1.example.com:9092,host2.example.com:9092\",\n            \"format\": \"csv\",\n            \"keyField\": \"key\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"purchase\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"key\\\",\\\"type\\\":\\\"bytes\\\"},\n                    {\\\"name\\\":\\\"user\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"item\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"count\\\",\\\"type\\\":\\\"int\\\"},\n                    {\\\"name\\\":\\\"price\\\",\\\"type\\\":\\\"double\\\"}\n                ]\n            }\"\n        }\n    }\n\nFor each Kafka message read, it will output a record with the schema:\n\n    +================================+\n    | field name  | type             |\n    +================================+\n    | key         | bytes            |\n    | user        | string           |\n    | item        | string           |\n    | count       | int              |\n    | price       | double           |\n    +================================+\n    ",
    "doc.Kafka-streamingsource": "# Kafka Streaming Source\n\n\nDescription\n-----------\nKafka streaming source. Emits a record with the schema specified by the user. If no schema\nis specified, it will emit a record with two fields: 'key' (nullable string) and 'message'\n(bytes). Kafka source is compatible with Kafka 0.8, 0.9 and 0.10\n\n\nUse Case\n--------\nThis source is used whenever you want to read from Kafka. For example, you may want to read messages\nfrom Kafka and write them to a Table.\n\n\nProperties\n----------\n**referenceName:** This will be used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**brokers:** List of Kafka brokers specified in host1:port1,host2:port2 form. (Macro-enabled)\n\n**topic:** The Kafka topic to read from. (Macro-enabled)\n\n**partitions:** List of topic partitions to read from. If not specified, all partitions will be read. (Macro-enabled)\n\n**defaultInitialOffset:** The default initial offset for all topic partitions.\nAn offset of -2 means the smallest offset. An offset of -1 means the latest offset. Defaults to -1.\nOffsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read.\nIf you wish to set different initial offsets for different partitions, use the initialPartitionOffsets property. (Macro-enabled)\n\n**initialPartitionOffsets:** The initial offset for each topic partition. If this is not specified,\nall partitions will use the same initial offset, which is determined by the defaultInitialOffset property.\nAny partitions specified in the partitions property, but not in this property will use the defaultInitialOffset.\nAn offset of -2 means the smallest offset. An offset of -1 means the latest offset.\nOffsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. (Macro-enabled)\n\n**schema:** Output schema of the source. If you would like the output records to contain a field with the\nKafka message key, the schema must include a field of type bytes or nullable bytes, and you must set the\nkeyField property to that field's name. Similarly, if you would like the output records to contain a field with\nthe timestamp of when the record was read, the schema must include a field of type long or nullable long, and you\nmust set the timeField property to that field's name. Any field that is not the timeField or keyField will be used\nin conjuction with the format to parse Kafka message payloads.\n\n**format:** Optional format of the Kafka event message. Any format supported by CDAP is supported.\nFor example, a value of 'csv' will attempt to parse Kafka payloads as comma-separated values.\nIf no format is given, Kafka message payloads will be treated as bytes.\n\n**timeField:** Optional name of the field containing the read time of the batch.\nIf this is not set, no time field will be added to output records.\nIf set, this field must be present in the schema property and must be a long.\n\n**keyField:** Optional name of the field containing the message key.\nIf this is not set, no key field will be added to output records.\nIf set, this field must be present in the schema property and must be bytes.\n\n**partitionField:** Optional name of the field containing the partition the message was read from.\nIf this is not set, no partition field will be added to output records.\nIf set, this field must be present in the schema property and must be an int.\n\n**offsetField:** Optional name of the field containing the partition offset the message was read from.\nIf this is not set, no offset field will be added to output records.\nIf set, this field must be present in the schema property and must be a long.\n\n**maxRatePerPartition:** Maximum number of records to read per second per partition. Defaults to 1000.\n\n\nExample\n-------\nThis example reads from the 'purchases' topic of a Kafka instance running\non brokers host1.example.com:9092 and host2.example.com:9092. The source will add\na time field named 'readTime' that contains a timestamp corresponding to the micro\nbatch when the record was read. It will also contain a field named 'key' which will have\nthe message key in it. It parses the Kafka messages using the 'csv' format\nwith 'user', 'item', 'count', and 'price' as the message schema.\n\n    {\n        \"name\": \"Kafka\",\n        \"type\": \"streamingsource\",\n        \"properties\": {\n            \"topics\": \"purchases\",\n            \"brokers\": \"host1.example.com:9092,host2.example.com:9092\",\n            \"format\": \"csv\",\n            \"timeField\": \"readTime\",\n            \"keyField\": \"key\",\n            \"schema\": \"{\n                \\\"type\\\":\\\"record\\\",\n                \\\"name\\\":\\\"purchase\\\",\n                \\\"fields\\\":[\n                    {\\\"name\\\":\\\"readTime\\\",\\\"type\\\":\\\"long\\\"},\n                    {\\\"name\\\":\\\"key\\\",\\\"type\\\":\\\"bytes\\\"},\n                    {\\\"name\\\":\\\"user\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"item\\\",\\\"type\\\":\\\"string\\\"},\n                    {\\\"name\\\":\\\"count\\\",\\\"type\\\":\\\"int\\\"},\n                    {\\\"name\\\":\\\"price\\\",\\\"type\\\":\\\"double\\\"}\n                ]\n            }\"\n        }\n    }\n\nFor each Kafka message read, it will output a record with the schema:\n\n    +================================+\n    | field name  | type             |\n    +================================+\n    | readTime    | long             |\n    | key         | bytes            |\n    | user        | string           |\n    | item        | string           |\n    | count       | int              |\n    | price       | double           |\n    +================================+\n\nNote that the readTime field is not derived from the Kafka message, but from the time that the\nmessage was read.\n",
    "doc.KAFKABATCHSOURCE": "[![Build Status](https://travis-ci.org/hydrator/kafka-plugins.svg?branch=master)](https://travis-ci.org/hydrator/kafka-plugins) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nKafka Batch Source\n===========\n\nKafka batch source that emits a records with user specified schema.\n\n<img align=\"center\" src=\"kafka-batch-source-plugins-config.png\"  width=\"400\" alt=\"plugin configuration\" />\n\nUsage Notes\n-----------\n\nKafka Streaming Source can be used to read events from a kafka topic. It uses kafka consumer [0.8.2 apis](https://kafka.apache.org/082/documentation.html) to read events from a kafka topic. Kafka Batch Source converts incoming kafka events into cdap structured records which then can be used for further transformations. \n\nThe source will read from the earliest available offset or the initial offset that specified in the config for the first run, remember the last offset it read last run and continue from that offset for the next run. \n\nPlugin Configuration\n---------------------\n\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :------ | :---------- |\n| **Kafka Brokers** | **Y** | N/A | List of Kafka brokers specified in host1:port1,host2:port2 form. |\n| **Kafka Topic** | **Y** | N/A | The Kafka topic to read from. |\n| **Offset Table Name** | **Y** | __kafka_batch_source_offsets | Optional table name to track the latest offsets read from Kafka. Stores offsets using a composite key of reference name, topic name and partition. Only change this table name if you would like to ignore the offsets read from a Kafka topic by a previous run using the Kafka Batch Source. | \n| **Topic Partition** | **N** | N/A | List of topic partitions to read from. If not specified, all partitions will be read.  |\n| **Initial Partition Offsets** | **N** | N/A | The initial offset for each topic partition. If this is not specified, earliest offset will be used. This offset will only be used for the first run of the pipeline. Any subsequent run will read from the latest offset from previous run.  Offsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. |\n| **Key Field** | **N** | N/A | Optional name of the field containing the message key. If this is not set, no key field will be added to output records. If set, this field must be present in the schema property and must be bytes. |\n| **Partition Field** | **N** | N/A | Optional name of the field containing the partition the message was read from. If this is not set, no partition field will be added to output records. If set, this field must be present in the schema property and must be an int. |\n| **Offset Field** | **N** | N/A | Optional name of the field containing the partition offset the message was read from. If this is not set, no offset field will be added to output records. If set, this field must be present in the schema property and must be a long. |\n| **Format** | **N** | N/A | Optional format of the Kafka event message. Any format supported by CDAP is supported. For example, a value of 'csv' will attempt to parse Kafka payloads as comma-separated values. If no format is given, Kafka message payloads will be treated as bytes. |\n\n\nBuild\n-----\nTo build this plugin:\n\n```\n   mvn clean package\n```    \n\nThe build will create a .jar and .json file under the ``target`` directory.\nThese files can be used to deploy your plugins.\n\nDeployment\n----------\nYou can deploy your plugins using the CDAP CLI:\n\n    > load artifact <target/kafka-plugins-<version>.jar config-file <target/kafka-plugins<version>.json>\n\nFor example, if your artifact is named 'kafka-plugins-<version>':\n\n    > load artifact target/kafka-plugins-<version>.jar config-file target/kafka-plugins-<version>.json\n    \n## Mailing Lists\n\nCDAP User Group and Development Discussions:\n\n* `cdap-user@googlegroups.com <https://groups.google.com/d/forum/cdap-user>`\n\nThe *cdap-user* mailing list is primarily for users using the product to develop\napplications or building plugins for appplications. You can expect questions from \nusers, release announcements, and any other discussions that we think will be helpful \nto the users.\n\n## License and Trademarks\n\nCopyright © 2017 Cask Data, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\nin compliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the \nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \neither express or implied. See the License for the specific language governing permissions \nand limitations under the License.\n\nCask is a trademark of Cask Data, Inc. All rights reserved.\n\nApache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with\npermission. No endorsement by The Apache Software Foundation is implied by the use of these marks.      \n",
    "doc.KAFKASOURCE": "[![Build Status](https://travis-ci.org/hydrator/kafka-plugins.svg?branch=master)](https://travis-ci.org/hydrator/kafka-plugins) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nKafka Source\n===========\n\nKafka streaming source that emits a records with user specified schema.\n\n<img align=\"center\" src=\"kafka-source-plugin-config.png\"  width=\"400\" alt=\"plugin configuration\" />\n\nUsage Notes\n-----------\n\nKafka Streaming Source can be used to read events from a kafka topic. It uses kafka consumer [0.8.2 apis](https://kafka.apache.org/082/documentation.html) to read events from a kafka topic. Kafka Source converts incoming kafka events into cdap structured records which then can be used for further transformations. \n\nThe source provides capabilities to read from latest offset or from beginning or from the provided kafka offset. The plugin relies on Spark Streaming offset [storage capabilities](https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html) to manager offsets and checkpoints.\n\nPlugin Configuration\n---------------------\n\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :----- | :---------- |\n| **Kafka Brokers** | **Y** | N/A | List of Kafka brokers specified in host1:port1,host2:port2 form. |\n| **Kafka Topic** | **Y** | N/A | The Kafka topic to read from. |\n| **Topic Partition** | **N** | N/A | List of topic partitions to read from. If not specified, all partitions will be read.  |\n| **Default Initial Offset** | **N** | N/A | The default initial offset for all topic partitions. An offset of -2 means the smallest offset. An offset of -1 means the latest offset. Defaults to -1. Offsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. If you wish to set different initial offsets for different partitions, use the initialPartitionOffsets property. |\n| **Initial Partition Offsets** | **N** | N/A | The initial offset for each topic partition. If this is not specified, all partitions will use the same initial offset, which is determined by the defaultInitialOffset property. Any partitions specified in the partitions property, but not in this property will use the defaultInitialOffset. An offset of -2 means the smallest offset. An offset of -1 means the latest offset. Offsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. |\n| **Time Field** | **N** | N/A | Optional name of the field containing the read time of the batch. If this is not set, no time field will be added to output records. If set, this field must be present in the schema property and must be a long. |\n| **Key Field** | **N** | N/A | Optional name of the field containing the message key. If this is not set, no key field will be added to output records. If set, this field must be present in the schema property and must be bytes. |\n| **Partition Field** | **N** | N/A | Optional name of the field containing the partition the message was read from. If this is not set, no partition field will be added to output records. If set, this field must be present in the schema property and must be an int. |\n| **Offset Field** | **N** | N/A | Optional name of the field containing the partition offset the message was read from. If this is not set, no offset field will be added to output records. If set, this field must be present in the schema property and must be a long. |\n| **Format** | **N** | N/A | Optional format of the Kafka event message. Any format supported by CDAP is supported. For example, a value of 'csv' will attempt to parse Kafka payloads as comma-separated values. If no format is given, Kafka message payloads will be treated as bytes. |\n\n\nBuild\n-----\nTo build this plugin:\n\n```\n   mvn clean package\n```    \n\nThe build will create a .jar and .json file under the ``target`` directory.\nThese files can be used to deploy your plugins.\n\nDeployment\n----------\nYou can deploy your plugins using the CDAP CLI:\n\n    > load artifact <target/kafka-plugins-<version>.jar config-file <target/kafka-plugins<version>.json>\n\nFor example, if your artifact is named 'kafka-plugins-<version>':\n\n    > load artifact target/kafka-plugins-<version>.jar config-file target/kafka-plugins-<version>.json\n    \n## Mailing Lists\n\nCDAP User Group and Development Discussions:\n\n* `cdap-user@googlegroups.com <https://groups.google.com/d/forum/cdap-user>`\n\nThe *cdap-user* mailing list is primarily for users using the product to develop\napplications or building plugins for appplications. You can expect questions from \nusers, release announcements, and any other discussions that we think will be helpful \nto the users.\n\n## License and Trademarks\n\nCopyright © 2017 Cask Data, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\nin compliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the \nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \neither express or implied. See the License for the specific language governing permissions \nand limitations under the License.\n\nCask is a trademark of Cask Data, Inc. All rights reserved.\n\nApache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with\npermission. No endorsement by The Apache Software Foundation is implied by the use of these marks.      \n",
    "doc.KAFKAWRITER-SINK": "[![Build Status](https://travis-ci.org/hydrator/kafka-plugins.svg?branch=master)](https://travis-ci.org/hydrator/kafka-plugins) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\nKafka Sink\n==========\n\nKafka sink that allows you to write events into CSV or JSON to kafka. Plugin has the capability to push the data to one or more Kafka topics. \nIt can use one of the field values from input to partition the data on topic. The sink can also be configured to operate in either sync or async mode. \n\n<img align=\"center\" src=\"kafka-sink-plugin-config.png\"  width=\"400\" alt=\"plugin configuration\" />\n\nUsage Notes\n-----------\n\nKafka sink emits events in realtime to configured kafka topic and partition. It uses kafka producer [0.8.2 apis](https://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html) to write events into kafka. \n\nThis sink can be configured to operate in synchronous or asynchronous mode. In synchronous mode, each event will be sent to the broker synchronously on the thread that calls it. This is no sufficient on most of the high volume environments. \nIn async mode, the kafka producer will batch together all the kafka events for greater throughput. But that makes it open for the possibility of dropping unsent events in case of client machine failure. Since kafka producer by default uses synchronous mode, this sink also uses Synchronous producer by default.\n\nIt uses String partitioner and String serializer for key and value to write events to kafka. Optionally if kafka key is provided, producer will use that key to partition events accross multiple partitions in a given topic. This sink also allows compression configuration. By default compression is none.\n\nKafka producer can be tuned using many properties as shown [here](https://kafka.apache.org/082/javadoc/org/apache/kafka/clients/producer/ProducerConfig.html). This sink allows user to configure any property supported by kafka 0.8.2 Producer.\n\n\nPlugin Configuration\n---------------------\n\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :----- | :---------- |\n| **Kafka Brokers** | **Y** | N/A | List of Kafka brokers specified in host1:port1,host2:port2 form. |\n| **Kafka Topic** | **Y** | N/A | The Kafka topic to write to. This should be a valid kafka topic string. Kafka topic should already exist. |\n| **Is Async** | **Y** | False | Specifies whether writing the events to broker is *Asynchronous* or *Synchronous*.  |\n| **Compression Type** | **Y** | none | This configuration specifies the format of the event published to Kafka. |\n| **Additional Kafka Producer Properties** | **N** | N/A | Specifies additional kafka producer properties like acks, client.id as key and value pair. |\n| **Message Format** | **Y** | CSV | This configuration specifies serialization format of the event published to Kafka. |\n| **Message Key Field** | **N** | N/A | This configuration specifies the input field that should be used as the key for the event published into Kafka. This field will be used to partition kafka events across multiple partitions of a topic. Key field should be of type string. |\n\n\nBuild\n-----\nTo build this plugin:\n\n```\n   mvn clean package\n```    \n\nThe build will create a .jar and .json file under the ``target`` directory.\nThese files can be used to deploy your plugins.\n\nDeployment\n----------\nYou can deploy your plugins using the CDAP CLI:\n\n    > load artifact <target/kafka-plugins-<version>.jar config-file <target/kafka-plugins<version>.json>\n\nFor example, if your artifact is named 'kafka-plugins-<version>':\n\n    > load artifact target/kafka-plugins-<version>.jar config-file target/kafka-plugins-<version>.json\n    \n## Mailing Lists\n\nCDAP User Group and Development Discussions:\n\n* `cdap-user@googlegroups.com <https://groups.google.com/d/forum/cdap-user>`\n\nThe *cdap-user* mailing list is primarily for users using the product to develop\napplications or building plugins for appplications. You can expect questions from \nusers, release announcements, and any other discussions that we think will be helpful \nto the users.\n\n## License and Trademarks\n\nCopyright © 2017 Cask Data, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\nin compliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the \nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \neither express or implied. See the License for the specific language governing permissions \nand limitations under the License.\n\nCask is a trademark of Cask Data, Inc. All rights reserved.\n\nApache, Apache HBase, and HBase are trademarks of The Apache Software Foundation. Used with\npermission. No endorsement by The Apache Software Foundation is implied by the use of these marks.      \n"
  }
}
