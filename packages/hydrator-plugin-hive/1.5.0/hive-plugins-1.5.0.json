{
  "parents": [
    "system:cdap-etl-batch[4.0.0,4.1.0)",
    "system:cdap-data-pipeline[4.0.0,4.1.0)"
  ],
  "properties": {
    "widgets.Hive-batchsink": "{\"metadata\": {\"spec-version\": \"1.0\"},\"configuration-groups\": [{\"label\": \"Hive Configuration\",\"properties\": [{\"widget-type\": \"textbox\",\"label\": \"Reference Name\",\"name\": \"referenceName\"},{\"widget-type\": \"textbox\",\"label\": \"Hive Metastore URI\",\"name\": \"metastoreURI\"},{\"widget-type\": \"textbox\",\"label\": \"Hive Database Name\",\"name\": \"databaseName\"},{\"widget-type\": \"textbox\",\"label\": \"Hive Table Name\",\"name\": \"tableName\"},{\"widget-type\": \"javascript-editor\",\"label\": \"Hive Partition Filter\",\"name\": \"partitions\"}]}],\"outputs\": [{\"name\": \"schema\",\"label\": \"schema\",\"widget-type\": \"schema\",\"widget-attributes\": {\"schema-types\": [\"boolean\",\"int\",\"long\",\"float\",\"double\",\"bytes\",\"string\"],\"schema-default-type\": \"string\"}}]}",
    "widgets.Hive-batchsource": "{\"metadata\": {\"spec-version\": \"1.0\"},\"configuration-groups\": [{\"label\": \"Hive Configuration\",\"properties\": [{\"widget-type\": \"textbox\",\"label\": \"Reference Name\",\"name\": \"referenceName\"},{\"widget-type\": \"textbox\",\"label\": \"Hive Metastore URI\",\"name\": \"metastoreURI\"},{\"widget-type\": \"textbox\",\"label\": \"Hive Database Name\",\"name\": \"databaseName\"},{\"widget-type\": \"textbox\",\"label\": \"Hive Table Name\",\"name\": \"tableName\"},{\"widget-type\": \"javascript-editor\",\"label\": \"Hive Partition Filter\",\"name\": \"partitions\"}]}],\"outputs\": [{\"name\": \"schema\",\"label\": \"schema\",\"widget-type\": \"schema\",\"widget-attributes\": {\"schema-types\": [\"boolean\",\"int\",\"long\",\"float\",\"double\",\"bytes\",\"string\"],\"schema-default-type\": \"string\"}}]}",
    "doc.Hive-batchsink": "# Hive Batch Sink\n\n\nDescription\n-----------\nConverts a StructuredRecord to a HCatRecord and then writes it to an existing Hive table.\n\n\nConfiguration\n-------------\n**referenceName:** This will be used to uniquely identify this sink for lineage, annotating metadata, etc.\n\n**metastoreURI:** The URI of Hive metastore in the format ``thrift://<hostname>:<port>``.\nExample: ``thrift://somehost.net:9083``.\n\n**tableName:** The name of the Hive table. This table must exist.\n\n**databaseName:** The name of the database. Defaults to default.\n\n**partitions:** Optional Hive expression filter for writing, provided as a JSON Map of key-value pairs that describe all of the\npartition keys and values for that partition. For example: if the partition column is type then this property\nshould be specified as ``{\"type\": \"typeOne\"}``.\nTo write multiple partitions simultaneously you can leave this empty; but all of the partitioning columns must\nbe present in the data you are writing to the sink.\n\n**schema:** Optional schema to use while writing to the Hive table. If no schema is provided, then the schema of the\ntable will be used and it should match the schema of the data being written.\n",
    "doc.Hive-batchsource": "# Hive Batch Source\n\n\nDescription\n-----------\nReads records from a Hive table and converts each record into a StructuredRecord with the help\nof the specified schema (if provided) or the schema of table.\n\n\nConfiguration\n-------------\n**referenceName:** This will be used to uniquely identify this source for lineage, annotating metadata, etc.\n\n**metastoreURI:** The URI of Hive metastore in the format of ``thrift://<hostname>:<port>``.\nExample: ``thrift://somehost.net:9083``.\n\n**tableName:** The name of the Hive table. This table must exist.\n\n**databaseName:** The name of the database. Defaults to default.\n\n**partitions:** Optional Hive expression filter for scan. This filter must only reference partition columns.\nValues from other columns will cause the pipeline to fail.\n\n**schema:** Optional schema to use while reading from the Hive table. If no schema is provided, then the schema of the\ntable will be used. Note: if you want to use a Hive table which has non-primitive types as a source, then you\nshould provide a schema with all non-primitive fields dropped, otherwise your pipeline will fail."
  }
}
