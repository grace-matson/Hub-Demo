{
  "parents": [
    "system:cdap-data-streams[4.1.0-SNAPSHOT,10.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.CDCDatabase-streamingsource": "{\"metadata\":{\"spec-version\":\"1.3\"},\"configuration-groups\":[{\"label\":\"Kafka Configuration\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"textbox\",\"label\":\"Kafka Broker\",\"name\":\"broker\"},{\"widget-type\":\"textbox\",\"label\":\"Kafka Topic\",\"name\":\"topic\"},{\"widget-type\":\"textbox\",\"label\":\"Default Initial Offset\",\"name\":\"defaultInitialOffset\"},{\"widget-type\":\"textbox\",\"label\":\"Max Rate Per Partition\",\"name\":\"maxRatePerPartition\",\"widget-attributes\":{\"default\":\"1000\"}}]}],\"outputs\":[{\"widget-type\":\"non-editable-schema-editor\",\"schema\":{\"type\":\"record\",\"name\":\"etlSchemaBody\",\"fields\":[{\"name\":\"op_type\",\"type\":[\"string\",\"null\"]},{\"name\":\"table\",\"type\":\"string\"},{\"name\":\"primary_keys\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"schema\",\"type\":\"string\"},{\"name\":\"change\",\"type\":{\"type\":\"record\",\"name\":\"changerecord\",\"fields\":[{\"name\":\"columns\",\"type\":{\"type\":\"map\",\"keys\":\"string\",\"values\":\"string\"}}]}}]}}]}",
    "widgets.CDCHBase-sparksink": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"HBase Sink Configuration\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\",\"description\":\"Reference specifies the name to be used to track this external source\"}]}]}",
    "widgets.CDCKudu-sparksink": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"Kudu Configuration\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\",\"description\":\"Reference specifies the name to be used to track this external source\"},{\"widget-type\":\"textbox\",\"label\":\"Master Addresses\",\"name\":\"master\",\"description\":\"Comma separated list of hostname:port of Apache Kudu Masters\"}]},{\"label\":\"Kudu Advanced Options\",\"properties\":[{\"widget-type\":\"number\",\"label\":\"No of buckets (DEFAULT: 16)\",\"name\":\"buckets\",\"widget-attributes\":{\"default\":16}},{\"widget-type\":\"number\",\"label\":\"Seed to randomize the mapping of rows to hash buckets (DEFAULT: 0)\",\"name\":\"seed\",\"widget-attributes\":{\"default\":1}},{\"widget-type\":\"select\",\"label\":\"Compression Algorithm. All fields will be applied same compression. (DEFAULT: Snappy)\",\"name\":\"compression-algo\",\"widget-attributes\":{\"values\":[\"Snappy\",\"LZ4\",\"ZLib\",\"Backend configured\",\"No Compression\"],\"default\":\"Snappy\"}},{\"widget-type\":\"select\",\"label\":\"Encoding Type. All fields will be applied same encoding. (DEFAULT : Auto)\",\"name\":\"encoding\",\"widget-attributes\":{\"values\":[\"Auto\",\"Plain\",\"Prefix\",\"Group Variant\",\"RLE\",\"Dictionary\",\"Bit Shuffle\"],\"default\":\"Auto\"}},{\"widget-type\":\"number\",\"label\":\"User operations timeout in milliseconds (DEFAULT: 30000ms)\",\"name\":\"opt-timeout\",\"widget-attributes\":{\"default\":30000}},{\"widget-type\":\"number\",\"label\":\"Administration operation timeout in milliseconds (DEFAULT: 30000ms)\",\"name\":\"admin-timeout\",\"widget-attributes\":{\"default\":30000}},{\"widget-type\":\"number\",\"label\":\"Number of copies (DEFAULT: 1)\",\"name\":\"replicas\",\"widget-attributes\":{\"default\":1}},{\"widget-type\":\"number\",\"label\":\"Rows to be cached before being flushed (DEFAULT: 1000)\",\"name\":\"row-flush\",\"widget-attributes\":{\"default\":1000}},{\"widget-type\":\"number\",\"label\":\"Specifies the number of boss threads to be used by the client.\",\"name\":\"boss-threads\",\"widget-attributes\":{\"default\":1}}]}]}",
    "widgets.CTSQLServer-streamingsource": "{\"metadata\":{\"spec-version\":\"1.3\"},\"configuration-groups\":[{\"label\":\"CT SQL Server Configuration\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Reference Name\",\"name\":\"referenceName\"},{\"widget-type\":\"textbox\",\"label\":\"Hostname\",\"name\":\"hostname\"},{\"widget-type\":\"textbox\",\"label\":\"Port\",\"name\":\"port\",\"widget-attributes\":{\"default\":\"1433\"}},{\"widget-type\":\"textbox\",\"label\":\"Username\",\"name\":\"username\"},{\"widget-type\":\"password\",\"label\":\"Password\",\"name\":\"password\"},{\"widget-type\":\"textbox\",\"label\":\"Database name\",\"name\":\"dbname\"}]}],\"outputs\":[{\"widget-type\":\"non-editable-schema-editor\",\"schema\":{\"name\":\"etlSchemaBody\",\"type\":\"record\",\"fields\":[{\"name\":\"message\",\"type\":[\"bytes\",\"null\"]}]}}]}",
    "widgets.GoldenGateNormalizer-transform": "{\"metadata\":{\"spec-version\":\"1.3\"},\"configuration-groups\":[{\"label\":\"Normalizer Configuration\",\"properties\":[{\"widget-type\":\"select\",\"label\":\"Include namespace name in table name\",\"name\":\"includeNamespaceInTableName\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}}]}],\"outputs\":[{\"widget-type\":\"non-editable-schema-editor\",\"schema\":{\"type\":\"record\",\"name\":\"etlSchemaBody\",\"fields\":[{\"name\":\"op_type\",\"type\":[\"string\",\"null\"]},{\"name\":\"table\",\"type\":\"string\"},{\"name\":\"primary_keys\",\"type\":{\"type\":\"array\",\"items\":\"string\"}},{\"name\":\"schema\",\"type\":\"string\"},{\"name\":\"change\",\"type\":{\"type\":\"record\",\"name\":\"changerecord\",\"fields\":[{\"name\":\"columns\",\"type\":{\"type\":\"map\",\"keys\":\"string\",\"values\":\"string\"}}]}}]}}]}",
    "doc.CDCDatabase-streamingsource": "CDC Database Source\n===================\n\nCDAP plugin for reading from the Kafka topic to which Oracle GoldenGate pushes the change schema and data.\nThis plugin is used in realtime data pipelines only.\n\nUsage Notes\n-----------\nThe plugin is configured to pull the changes from the Kafka topic to which Oracle GoldenGate pushes the change schema\nand data. Note that the kafka topic should have one partition and GoldenGate should be configured to emit the schema as\nwell as data to the same topic. This guarantees the ordering of the events as they occurs in the source database.\nThis plugin can be used along with CDC sink plugins such as `CDC Kudu`, and `CDC HBase`.\n\n\nPlugin Configuration\n---------------------\n| Config | Required | Default | Description |\n| :------------ | :------: | :----- | :---------- |\n| **Reference Name** | **Y** | N/A | A unique name which will be used to identify this source for lineage and annotating metadata.|\n| **Kafka Broker** | **Y** | N/A | Kafka broker specified in host:port form. For example, example.com:9092  |\n| **Kafka Topic** | **Y** | N/A | Name of the topic to which Oracle GoldenGate is configured to publish schema and data changes. This topic should have single partition to maintain the ordering between the published messages. |\n| **Default Initial Offser** | **N** | -1 | The default initial offset for all topic partitions. An offset of -2 means the smallest offset. An offset of -1 means the latest offset. Defaults to -1. Offsets are inclusive. If an offset of 5 is used, the message at offset 5 will be read. |\n| **Max Rate Per Partition** | **N** | 1000 | Maximum number of records to read per second per partition. 0 means there is no limit. Defaults to 1000. |\n",
    "doc.CDCHBase-sparksink": "CDC HBase Sink\n==============\n\nCDAP plugin for storing the changed data into HBase. This plugin can be used with\neither `CDC Database Source` or `ChangeTrackingSQLServer Source` plugin for reading the changes\nfrom Oracle or SQLServer databases respectively.\n\nUsage Notes\n-----------\n\nThe plugin accepts the DDL and DML records from the source plugin to which it is connected to. If the table does not\nexists in the HBase cluster, it will be created.\n\nPlugin Configuration\n---------------------\n\nThis plugin only accepts the reference name. Other configurations required to connect to the HBase cluster are\npicked up from hbase-site.xml.\n\n| Config | Required | Default | Description |\n| :------------ | :------: | :----- | :---------- |\n| **Reference Name** | **Y** | N/A | A unique name which will be used to identify this source for lineage and annotating metadata.|\n",
    "doc.CDCKudu-sparksink": "CDC Kudu Sink\n=============\n\nCDAP plugin for storing the changed data into Kudu table. This plugin can be used with\neither `CDC Database Source` or `ChangeTrackingSQLServer Source` plugin for\nreading the changes from Oracle or SQLServer databases respectively.\n\nUsage Notes\n-----------\nThe plugin accepts the DDL and DML records from the source plugin to which it is connected to. If the table does not\nexists in the Kudu cluster, it will be created. DDL operations such as `add column` or `remove column` will\nresult in the corresponding schema changes in the Kudu table. DML operations such as `Insert`, `Update`, or `Delete`\nresults into updates of the corresponding record in the Kudu table.\n\nPlugin Configuration\n--------------------\n\n| Config | Required | Default | Description |\n| :------------ | :------: | :----- | :---------- |\n| **Reference Name** | **Y** | N/A | A unique name which will be used to identify this source for lineage and annotating metadata.|\n| **Kudu Master Host** | **Y** | N/A | Specifies the list of Kudu master hosts that this plugin will attempt connect to. It's a comma separated list of &lt;hostname&gt;:&lt;port&gt;. Connection is attempt after the plugin is initialized in the pipeline.  |\n| **No of Buckets** | N | 16 | Number of buckets the keys are split into |\n| **Hash seed** | N | 1 | The seed value specified is used to randomize mapping of rows to hash buckets. Setting the seed will ensure the hashed columns contain user provided values.|\n| **Compression Algorithm** | N | Snappy | Specifies the compression algorithm to be used for the columns. Following are different options available. |\n| **Encoding** | N | Auto Encoding | Specifies the block encoding for the column. Following are different options available.  |\n| **Operation Timeout** | N | 30000 | This configuration sets the timeout in milliseconds for user operations with Kudu. If you are writing large sized records it's recommended to increase the this time. It's defaulted to 30 seconds. |\n| **Admin Timeout** | N | 30000 | This configuration is used to set timeout in milliseconds for administrative operations like for creating table if table doesn't exist. This time is mainly used during initialize phase of the plugin when the table is created if it doesn't exist. |\n| **Number of replicas** | N | 1 | Specifies the number of replicas for the above table. This will specify the number of replicas that each tablet will have. By default it will use the default set on the server side and that is generally 1.|\n| **Rows to be cached** | N | 1000 | Specifies number of rows to be cached before being flushed |\n| **Boss Threads** | N | 1 | Number of boss threads used in the Kudu client to interact with Kudu backend. |\n\n",
    "doc.CTSQLServer": "Change Tracking SQL Server Streaming Source\n===========================================\n\nChange Tracking SQL Server Streaming Source allows you to perform EDW offload in realtime through Spark Streaming. \nIt takes advantage of the change tracking information to minimize the data transfer to keep the downstream dataset \nin-sync.\n\n## SQL Server Chnage Tracking\n\n \n"
  }
}
