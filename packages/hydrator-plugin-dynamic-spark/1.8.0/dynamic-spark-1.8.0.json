{
  "parents": [
    "system:cdap-data-pipeline[4.3.0-SNAPSHOT,10.0.0-SNAPSHOT)",
    "system:cdap-data-streams[4.3.0-SNAPSHOT,10.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.ScalaSparkCompute-sparkcompute": "{\"metadata\":{\"spec-version\":\"1.5\"},\"display-name\":\"Spark\",\"configuration-groups\":[{\"label\":\"Spark Scala\",\"properties\":[{\"widget-type\":\"scala-editor\",\"label\":\"Scala\",\"name\":\"scalaCode\",\"widget-attributes\":{\"default\":\"/**\\n * Transforms the provided input Apache Spark RDD or DataFrame into another RDD or DataFrame.\\n *\\n * The input DataFrame has the same schema as the input schema to this stage and the transform method should return a DataFrame that has the same schema as the output schema setup for this stage.\\n * To emit logs, use: \\n *     import org.slf4j.LoggerFactory\\n *     val logger = LoggerFactory.getLogger('mylogger')\\n *     logger.info('Logging')\\n *\\n *\\n * @param input the input DataFrame which has the same schema as the input schema to this stage.\\n * @param context a SparkExecutionPluginContext object that can be used to emit zero or more records (using the emitter.emit() method) or errors (using the emitter.emitError() method) \\n * @param context an object that provides access to:\\n *      1. CDAP Datasets and Streams - context.fromDataset('counts'); or context.fromStream('input');\\n *      2. Original Spark Context - context.getSparkContext();\\n *      3. Runtime Arguments - context.getArguments.get('priceThreshold')\\n */\\ndef transform(df: DataFrame, context: SparkExecutionPluginContext) : DataFrame = {\\n  df\\n}\"}}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"schema\",\"widget-attributes\":{\"schema-types\":[\"boolean\",\"int\",\"long\",\"float\",\"double\",\"bytes\",\"string\",\"map<string, string>\"],\"schema-default-type\":\"string\"}}]}",
    "widgets.ScalaSparkProgram-sparkprogram": "{\"metadata\":{\"spec-version\":\"1.5\"},\"display-name\":\"Spark Program\",\"configuration-groups\":[{\"label\":\"Spark Scala Program\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Main Class Name\",\"name\":\"mainClass\",\"widget-attributes\":{\"default\":\"SparkProgram\"}},{\"widget-type\":\"scala-editor\",\"label\":\"Scala\",\"name\":\"scalaCode\",\"widget-attributes\":{\"default\":\"import co.cask.cdap.api.spark._\\nimport org.apache.spark._\\nimport org.slf4j._\\n\\nclass SparkProgram extends SparkMain {\\n  import SparkProgram._\\n\\n  override def run(implicit sec: SparkExecutionContext): Unit = {\\n    LOG.info(\\\"Spark Program Started\\\")\\n\\n    val sc = new SparkContext\\n\\n    LOG.info(\\\"Spark Program Completed\\\")\\n  }\\n}\\n\\nobject SparkProgram {\\n  val LOG = LoggerFactory.getLogger(getClass())\\n}\"}},{\"widget-type\":\"dsv\",\"label\":\"Dependencies\",\"name\":\"dependencies\",\"widget-attributes\":{\"delimiter\":\",\"}}]}],\"outputs\":[]}",
    "doc.ScalaSparkCompute-sparkcompute": "# Spark Computation in Scala\n\nDescription\n-----------\nExecutes user-provided Spark code in Scala that transforms RDD to RDD with full\naccess to all Spark features.\n\nUse Case\n--------\nThis plugin can be used when you want to have complete control on the Spark computation.\nFor example, you may want to join the input RDD with another Dataset and select a subset\nof the join result using Spark SQL.\n\nProperties\n----------\n**scalaCode** Spark code in Scala defining how to transform RDD to RDD. \nThe code must implement a function called ``transform``, whose signature should be one of:\n\n    def transform(df: DataFrame) : DataFrame\n\n    def transform(df: DataFrame, context: SparkExecutionPluginContext) : DataFrame\n    \nThe input ``DataFrame`` has the same schema as the input schema to this stage and the ``transform`` method\nshould return a ``DataFrame`` that has the same schema as the output schema setup for this stage.\nUsing the ``SparkExecutionPluginContext``, you can access CDAP\nentities such as Stream and Dataset, as well as providing access to the underlying ``SparkContext`` in use.\n \nOperating on lower level ``RDD`` is also possible by using the one of the following forms of the ``transform`` method:\n\n    def transform(rdd: RDD[StructuredRecord]) : RDD[StructuredRecord]\n\n    def transform(rdd: RDD[StructuredRecord], context: SparkExecutionPluginContext) : RDD[StructuredRecord]\n   \nFor example:\n\n    def transform(rdd: RDD[StructuredRecord], context: SparkExecutionPluginContext) : RDD[StructuredRecord] = {\n      val outputSchema = context.getOutputSchema\n      rdd\n        .flatMap(_.get[String](\"body\").split(\"\\\\s+\"))\n        .map(s => (s, 1))\n        .reduceByKey(_ + _)\n        .map(t => StructuredRecord.builder(outputSchema).set(\"word\", t._1).set(\"count\", t._2).build)\n    }\n        \nThe will perform a word count on the input field ``'body'``, \nand produces records of two fields, ``'word'`` and ``'count'``.\n\nThe following imports are included automatically and are ready for the user code to use:\n\n      import co.cask.cdap.api.data.format._\n      import co.cask.cdap.api.data.schema._;\n      import co.cask.cdap.etl.api.batch._\n      import org.apache.spark._\n      import org.apache.spark.api.java._\n      import org.apache.spark.rdd._\n      import org.apache.spark.sql._\n      import org.apache.spark.SparkContext._\n      import scala.collection.JavaConversions._\n\n\n**schema:** The schema of output objects. If no schema is given, it is assumed that the output\nschema is the same as the input schema.",
    "doc.ScalaSparkProgram-sparkprogram": "# Spark Program in Scala\n\nDescription\n-----------\nExecutes user-provided Spark code in Scala.\n\nUse Case\n--------\nThis plugin can be used when you want arbitrary Spark code.\n\nProperties\n----------\n**mainClass** The fully qualified class name for the Spark application.\nIt must either be an ``object`` that has a ``main`` method define inside, with the method signature as \n``def main(args: Array[String]): Unit``; or it is a class that extends from the CDAP \n``co.cask.cdap.api.spark.SparkMain`` trait that implements the ``run`` method, with the method signature as\n``def run(implicit sec: SparkExecutionContext): Unit``\n\n**scalaCode** The self-contained Spark application written in Scala.\nFor example, an application that reads from CDAP stream with name ``streamName``, \nperforms a simple word count logic and logs the result can be written as:\n\n    import co.cask.cdap.api.spark._\n    import org.apache.spark._\n    import org.slf4j._\n\n    class SparkProgram extends SparkMain {\n      import SparkProgram._\n\n      override def run(implicit sec: SparkExecutionContext): Unit = {\n        val sc = new SparkContext\n        val result = sc.fromStream[String](\"streamName\")\n          .flatMap(_.split(\"\\\\s+\"))\n          .map((_, 1))\n          .reduceByKey(_ + _)\n          .collectAsMap\n          \n        LOG.info(\"Result is: {}\", result)\n      }\n    }\n\n    object SparkProgram {\n      val LOG = LoggerFactory.getLogger(getClass())\n    }\n \n**dependencies** Extra dependencies for the Spark program.\nIt is a ',' separated list of URI for the location of dependency jars.\nA path can be ended with an asterisk '*' as a wildcard, in which all files with extension '.jar' under the\nparent path will be included.\n \nPlease refer to the [CDAP documentation](https://docs.cask.co/cdap/current/en/developers-manual/building-blocks/spark-programs.html#cdap-spark-program) on the enhancements that CDAP brings to Spark."
  }
}
