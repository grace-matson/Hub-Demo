{
  "parents": [
    "system:cdap-etl-batch[4.0.0,10.0.0-SNAPSHOT)",
    "system:cdap-etl-realtime[4.0.0,10.0.0-SNAPSHOT)",
    "system:cdap-data-pipeline[4.0.0,10.0.0-SNAPSHOT)",
    "system:cdap-data-streams[4.0.0,10.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.S3ToRedshift-action": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"S3ToRedshift Configuration\",\"properties\":[{\"widget-type\":\"password\",\"label\":\"S3 Access Key\",\"name\":\"accessKey\"},{\"widget-type\":\"password\",\"label\":\"S3 Secret Access Key\",\"name\":\"secretAccessKey\"},{\"widget-type\":\"password\",\"label\":\"S3 IAM Role\",\"name\":\"iamRole\"},{\"widget-type\":\"select\",\"label\":\"S3 Region\",\"name\":\"s3Region\",\"widget-attributes\":{\"values\":[\"us-east-1\",\"us-west-1\",\"us-west-2\",\"ap-southeast-1\",\"ap-southeast-2\",\"ap-northeast-1\",\"eu-west-1\",\"sa-east-1\"]}},{\"widget-type\":\"textbox\",\"label\":\"S3 Data Path\",\"name\":\"s3DataPath\"},{\"widget-type\":\"textbox\",\"label\":\"JDBC Redshift Cluster Database URL\",\"name\":\"clusterDbUrl\"},{\"widget-type\":\"textbox\",\"label\":\"Redshift Master User\",\"name\":\"masterUser\"},{\"widget-type\":\"password\",\"label\":\"Redshift Master Password\",\"name\":\"masterPassword\"},{\"widget-type\":\"textbox\",\"label\":\"Redshift Table Name\",\"name\":\"tableName\"},{\"widget-type\":\"csv\",\"label\":\"List of Columns\",\"name\":\"listOfColumns\"}]}],\"outputs\":[]}",
    "doc.S3ToRedshift-action": "# S3ToRedshift Action\n\n\nDescription\n-----------\nS3ToRedshift Action that will load the data from AWS S3 bucket into the AWS Redshift table.\n\nProperties\n----------\n\n**accessKey:** Access key for AWS S3 to connect to. Either provide 'Keys(Access and Secret Access keys)' or 'IAM\nRole' for connecting to AWS S3 bucket. (Macro-enabled)\n\n**secretAccessKey:** Secret access key for AWS S3 to connect to. Either provide 'Keys(Access and Secret Access keys)'\n or 'IAM Role' for connecting to AWS S3 bucket. (Macro-enabled)\n\n**iamRole:** IAM Role for AWS S3 to connect to. This can only be used if the cluster is hosted on AWS servers. Either\n provide 'Keys(Access and Secret Access keys)' or 'IAM Role' for connecting to AWS S3 bucket. (Macro-enabled)\n\n**s3Region:** The region for AWS S3 to connect to. If not specified, then plugin will consider that S3 bucket is in\nthe same region as of the Redshift cluster. (Macro-enabled)\n\n**s3DataPath:** The S3 path of the bucket where the data is stored and will be loaded into the Redshift table.\nFor example, 's3://bucket-name/test/' or 's3://bucket-name/test/2017-02-22/'(will load files present in specific\ndirectory) or 's3://bucket-name/test'(will load the files having prefix ``test``) or\n's3://bucket-name/test/2017-02-22'(will load files from ``test`` directory having prefix ``2017-02-22``).\n(Macro-enabled)\n\n**clusterDbUrl:** The JDBC Redshift database URL for Redshift cluster, where the table is present. For example,\n'jdbc:redshift://x.y.us-west-2.redshift.amazonaws.com:5439/dev'. (Macro-enabled)\n\n**masterUser:** Master user for the Redshift cluster to connect to. (Macro-enabled)\n\n**masterPassword:** Master password for Redshift cluster to connect to. (Macro-enabled)\n\n**tableName:** The Redshift table name where the data from the S3 bucket will be loaded. (Macro-enabled)\n\n**listOfColumns:** Comma-separated list of the Redshift table column names to load the specific columns from S3\nbucket. If not provided, then all the columns from S3 will be loaded into the Redshift table. (Macro-enabled)\n\nConditions\n----------\nAny invalid configurations for connecting to AWS S3 bucket or Redshift cluster, will result into the runtime failure.\n\nBoth configurations 'Keys(Access and Secret Access keys)' and 'IAM Role' can not be provided or empty at the same\ntime. Either provide 'Keys(Access and Secret Access keys)' or 'IAM Role' for connecting to AWS S3 bucket.\n\nS3 data path should starts with s3://, not with s3n:// or s3a:// uri scheme.\n\nTable must exists in the Redshift cluster, for loading the data. If not, then it will result into the runtime failure.\n\nSchema of the table must match with S3 bucket data schema. If not, then it will result into the runtime failure.\n\nPlugin supports only avro formatted data present in the S3 bucket to be loaded into the Redshift table and uses\n'auto' option for formatting.\n\nExample\n-------\nThis example connects to a S3 instance using the 'accessKey and secretAccessKey', and to Redshift instance using\n'clusterDbUrl, masterUser and masterPassword'. Data from the S3 bucket provided through 's3DataPath' will be loaded\ninto the Redshift table 'redshifttest'.\n\n    {\n      \"name\": \"S3ToRedshift\",\n      \"type\": \"action\",\n        \"properties\": {\n          \"accessKey\": \"access-key\",\n          \"secretAccessKey\": \"secret-access-key\",\n          \"s3DataPath\": \"s3://bucket-name/test/\",\n          \"clusterDbUrl\": \"jdbc:redshift://x.y.us-west-2.redshift.amazonaws.com:5439/dev\",\n          \"masterUser\": \"master-user\",\n          \"masterPassword\": \"master-password\",\n          \"tableName\": \"redshifttest\"\n        }\n    }\n"
  }
}