{
  "parents": [
    "system:cdap-data-pipeline[4.1.0,4.2.0-SNAPSHOT)",
    "system:cdap-data-streams[4.1.0,4.2.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.RedshiftToS3-action": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"Basic\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Query\",\"name\":\"query\",\"widget-attributes\":{\"placeholder\":\"SELECT * FROM myTable\"}},{\"widget-type\":\"textbox\",\"label\":\"JDBC Redshift Cluster Database URL\",\"name\":\"redshiftClusterURL\",\"widget-attributes\":{\"placeholder\":\"jdbc:redshift://<endpoint-address>:<endpoint-port>/<db-name>\"}},{\"widget-type\":\"textbox\",\"label\":\"Redshift Master User\",\"name\":\"redshiftMasterUser\"},{\"widget-type\":\"password\",\"label\":\"Redshift Master Password\",\"name\":\"redshiftMasterPassword\"},{\"widget-type\":\"textbox\",\"label\":\"S3 Data Path\",\"name\":\"s3DataPath\",\"widget-attributes\":{\"placeholder\":\"s3://object-path/name-prefix\"}},{\"widget-type\":\"password\",\"label\":\"Access Key\",\"name\":\"accessKey\"},{\"widget-type\":\"password\",\"label\":\"Secret Access Key\",\"name\":\"secretAccessKey\"},{\"widget-type\":\"password\",\"label\":\"AWS IAM Role\",\"name\":\"iamRole\"}]},{\"label\":\"Advanced\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Output Path Token\",\"name\":\"outputPathToken\",\"widget-attributes\":{\"placeholder\":\"filePath\"}},{\"widget-type\":\"select\",\"label\":\"Manifest\",\"name\":\"manifest\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"textbox\",\"label\":\"S3 Delimiter\",\"name\":\"delimiter\",\"widget-attributes\":{\"placeholder\":\"|\"}},{\"widget-type\":\"select\",\"label\":\"Parallel\",\"name\":\"parallel\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"true\"}},{\"widget-type\":\"select\",\"label\":\"Compression\",\"name\":\"compression\",\"widget-attributes\":{\"values\":[\"NONE\",\"GZIP\",\"BZIP2\"],\"default\":\"NONE\"}},{\"widget-type\":\"select\",\"label\":\"Allow Overwrite\",\"name\":\"allowOverWrite\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"select\",\"label\":\"Add Quotes\",\"name\":\"addQuotes\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"select\",\"label\":\"Escape\",\"name\":\"escape\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}}]}],\"outputs\":[]}",
    "doc.RedshiftToS3-action": "# Redshift To S3 Action\n\n\nDescription\n-----------\nThe RedshiftToS3 Action runs the UNLOAD command on AWS to save the results of a query from Redshift to one or more \nfiles on Amazon Simple Storage Service (Amazon S3). For more information on the UNLOAD command, please see \nthe [AWS Documentation](http://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html).\n\n\nUse Case\n--------\nThis action is used whenever you need to quickly move data from Redshift to an S3 bucket before processing in a pipeline.\nFor example, a financial customer would like to quickly unload financial reports into S3 that have been generated from\nprocessing that is happening in Redshift. The pipeline would have a Redshift to S3 action at the beginning,\nand then leverage the S3 source to read that data into a processing pipeline.\n\nProperties\n----------\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :-----: | :---------- |\n| **Query** | **Y** | None | A SELECT query, the results of which are unloaded from Redshift table to the S3 bucket. | \n| **Redshift Cluster URL** | **Y** | None | JDBC Redshift DB url for connecting to the redshift cluster. The URL should include the port and database name. It should be in the format: ``jdbc:redshift://<endpoint-address>:<endpoint-port>/<db-name>``. This is only used to issue the UNLOAD command, not to execute the query. This plugin leverages the ``com.amazon.redshift.jdbc42.Driver`` for making connections. | \n| **Redshift Master User** | **Y** | None | Master user for the Redshift cluster to connect to.\n| **Redshift Master Password** | **Y** | None | Master password for Redshift cluster to connect to.\n| **S3 Data Path** | **Y** | None | The full path, including bucket name, to the location on Amazon S3 where Amazon Redshift will write the output file objects, including the manifest file if MANIFEST is specified. It should be in the format: ``s3://object-path/name-prefix``. This supports globbing syntax such as ``prefix-*``. To read an entire directory, ensure the path ends with a trailing ``/``.\n| **Access Key** | **N** | None | The Access Key provided by AWS so that the Redshift cluster can write to the S3 location.\n| **Secret Access Key** | **N** | None | AWS Secret Key provided by AWS so that the Redshift cluster can write to the S3 location.\n| **IAM Role** | **N** | None | IAM role having GET, LIST, and PUT permissions to the S3 bucket. The IAM Role should be in the form of ``arn:aws:iam::<aws-account-id>:role/<role-name>``. For more information about IAM Roles and the UNLOAD command, see the [AWS Documentation](http://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html).\n| **Output Path Token** | **N** | ``filePath`` | The macro key used to store the S3 file path for the unloaded file(s). Plugins that run at later stages in the pipeline can retrieve the file path using this key through macro substitution. For example, you could use ``${filePath}`` if ``filePath`` is the key specified.\n| **Create Manifest?** | **N** | false | Used to determine if a manifest file is to be created during the unload. The manifest file explicitly lists the data files that are created by the UNLOAD process. It will be created in the same directory as the UNLOADed files.\n| **Delimiter** | **N** | &#124; | Single ASCII character that is used to separate fields in the output file.\n| **Parallel** | **N** | true | Used to determine if UNLOAD writes data in parallel to multiple files, according to the number of slices in the cluster.\n| **Compression?** | **N** | NONE | Unloads data into one or more compressed files. Can be one of the following: NONE, BZIP2 or GZIP.\n| **Allow Overwrite?** | **N** | false | Used to determine if UNLOAD will overwrite existing files, including the manifest file, if the file is already available.\n| **Add Quotes?** | **N** | false | Used to determine if UNLOAD places quotation marks around each unloaded data field, so that Redshift can unload data values that contain the delimiter itself.\n| **Escape?** | **N** | false | Used to determine if escape character (\\\\) is to be placed before CHAR and VARCHAR columns in delimited unload file for the following characters: Linefeed ``\\n``, Carriage return ``\\r``, delimiter, escape character \\\\, and quote character: \" or '.\n\nUsage Notes\n-----------\n1. Both Access/Secret Keys and IAM Role should not be provided at the same time. Only one authentication mechanism should be provided.\n1. The Redshift table from which user wants to unload the data should already exist in the database specified by the ``Redshift Cluster URL``.\n1. The Amazon S3 bucket where Amazon Redshift will write the output files **must reside** in the same region as your cluster.\n1. S3 data path should start with ``s3://`` and not with the ``s3n://`` or ``s3a://`` URI scheme.\n"
  }
}