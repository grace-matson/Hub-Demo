{
  "properties": {
    "widgets.Run-transform": "{\n  \"outputs\": [{\n    \"name\": \"schema\",\n    \"widget-type\": \"schema\",\n    \"label\": \"schema\",\n    \"widget-attributes\": {\n      \"schema-types\": [\n        \"boolean\",\n        \"int\",\n        \"long\",\n        \"float\",\n        \"double\",\n        \"bytes\",\n        \"string\"\n      ],\n      \"schema-default-type\": \"string\",\n      \"property-watch\": \"format\"\n    }\n  }],\n  \"metadata\": {\"spec-version\": \"1.0\"},\n  \"configuration-groups\": [{\n    \"label\": \"Run Configuration\",\n    \"properties\": [\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"commandToExecute\",\n        \"label\": \"Command to Execute\"\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"fieldsToProcess\",\n        \"label\": \"Fields to Process for Variable Inputs\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Field Name\",\n          \"delimiter\": \",\"\n        }\n      },\n      {\n        \"widget-type\": \"csv\",\n        \"name\": \"fixedInputs\",\n        \"label\": \"Fixed Inputs\",\n        \"widget-attributes\": {\n          \"value-placeholder\": \"Input Value\",\n          \"delimiter\": \" \"\n        }\n      },\n      {\n        \"widget-type\": \"textbox\",\n        \"name\": \"outputField\",\n        \"label\": \"Output Field\"\n      },\n      {\n        \"widget-type\": \"select\",\n        \"name\": \"outputFieldType\",\n        \"label\": \"Output Field Type\",\n        \"widget-attributes\": {\"values\": [\n          \"boolean\",\n          \"bytes\",\n          \"double\",\n          \"float\",\n          \"int\",\n          \"long\",\n          \"string\"\n        ]}\n      }\n    ]\n  }]\n}",
    "doc.Run-transform": "# Run Transform\n\n\nDescription\n-----------\nRuns an executable binary which is installed and available on the local filesystem of the Hadoop nodes. Run transform\nplugin allows the user to read the structured record as input and returns the output record, to be further processed\ndownstream in the pipeline.\n\nUse Case\n--------\nIn enterprise, there are some existing tools or executable binaries that perform complex transformations of data.\nThis plugin can be used, when user would like to execute these type of binaries that will read the structured record\nas input, process it and retrieve the results back to the pipeline.\n\nProperties\n----------\n**commandToExecute:** Command that will contain the full path to the executable binary present on the local\nfilesystem of the Hadoop nodes as well as how to execute that binary. It should not contain any input arguments. For\nexample, java -jar /home/user/ExampleRunner.jar, if the binary to be executed is of type jar.\n\n**fieldsToProcess:** A comma-separated sequence of the fields that will be passed to the binary through STDIN as an\nvarying input. For example, 'firstname' or 'firstname,lastname' in case of multiple inputs. Please make sure that\n the sequence of fields is in the order as expected by binary. (Macro Enabled)\n\n**fixedInputs:** A space-separated sequence of the fixed inputs that will be passed to the executable binary\nthrough STDIN. Please make sure that the sequence of inputs is in the order as expected by binary. All the\nfixed inputs will be followed by the variable inputs, provided through 'Fields to Process for Variable Inputs'.\n(Macro enabled)\n\n**outputField:** The field name that holds the output of the executable binary.\n\n**outputFieldType:** Schema type of the 'Output Field'. Supported types are: boolean, bytes, double, float, int, long\nand string.\n\nConditions\n----------\nExecutable binary and its dependencies must be available on all the nodes of the Hadoop cluster, prior to the execution\nof binary.\n\nExecutable binary will always read the input through STDIN and should generate the STDOUT for each input record.\nAlso, errors emitted by the executable through STDERR will be captured in logs.\n\nSupported types for binary to be executed are: 'exe, sh, bat and jar'.\n\nPath to the executable binary, specified in 'commandToExecute' property, should be an absolute path not the URI\npath i.e. should not start with hdfs:// or file:///.\n\nExecutable binary can take 0 to N inputs. Source for the varying inputs will always be the structured records coming\nthrough the Hydrator source stage and will passed to the binary through STDIN. Required fields can be provided\nusing 'fieldsToProcess' property.\n\nFixed inputs (if any), will always be followed by the varying inputs. All the inputs will be passed as space\nseparated sequence to the executable binary through STDIN. This will be the format for sending the inputs to the\nexecutable binary.\n\nInputs should be in the expected order and the supported format. Any mismatch in the sequence will result into the\nruntime failure.\n\nExample\n-------\nThis example will run the executable binary of type jar: '/home/user/Permutations.jar' present on the local\nfilesystem of the Hadoop node. Executable binary will read the varying input through 'word' field coming from\nthe input record. Also, it will take some fixed inputs '50 true' for the processing. Output of the executable\nbinary will be saved in the 'permutation' field.\n\n    {\n      \"name\": \"Run\",\n      \"type\": \"transform\",\n        \"properties\": {\n          \"commandToExecute\": \"java -jar /home/user/Permutations.jar\",\n          \"fieldsToProcess\": \"word\",\n          \"fixedInputs\": \"50 true\",\n          \"outputField\": \"permutation\",\n          \"outputFieldType\": \"string\"\n        }\n    }\n\nFor example, suppose the Run transform receives the input record:\n\n    +=============================+\n    | id : STRING | word : STRING |\n    +=============================+\n    | W1          | AAC           |\n    | W2          | ABC           |\n    | W3          | AACE          |\n    +=============================+\n\nOutput records will contain all the input fields along with the output field 'permutation', that will be passed to the\nnext stage in the pipeline:\n\n    +========================================================================================================+\n    | id : STRING | word : STRING | permutation : STRING                                                     |\n    +========================================================================================================+\n    | W1          | AAC           | [AAC, ACA, CAA]                                                          |\n    | W2          | ABC           | [ACB, ABC, BCA, CBA, CAB, BAC]                                           |\n    | W3          | AACE          | [AACE, AAEC, ACAE, ACEA, AEAC, AECA, CAAE, CAEA, CEAA, EAAC, EACA, ECAA] |\n    +========================================================================================================+\n"
  },
  "parents": [
    "system:cdap-data-pipeline[6.0.0-SNAPSHOT,7.0.0-SNAPSHOT)",
    "system:cdap-data-streams[6.0.0-SNAPSHOT,7.0.0-SNAPSHOT)"
  ]
}