{
  "parents": [
    "system:cdap-data-pipeline[6.0.0-SNAPSHOT,7.0.0-SNAPSHOT)",
    "system:cdap-data-streams[6.0.0-SNAPSHOT,7.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.AzureFaceExtractor-transform": "{\"metadata\":{\"spec-version\":\"1.4\"},\"configuration-groups\":[{\"label\":\"Configuration Options\",\"properties\":[{\"widget-type\":\"input-field-selector\",\"label\":\"Source Field\",\"name\":\"sourceFieldName\"},{\"widget-type\":\"textbox\",\"label\":\"Azure Face API Key\",\"name\":\"facesSubscriptionKey\"},{\"widget-type\":\"textbox\",\"label\":\"Azure Emotion API Key\",\"name\":\"emotionSubscriptionKey\"},{\"widget-type\":\"select\",\"label\":\"Continue Processing If There Are Errors?\",\"name\":\"continueOnError\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}}]}],\"outputs\":[{\"name\":\"schema\",\"widget-type\":\"non-editable-schema-editor\",\"schema\":{\"name\":\"etlSchemaBody\",\"type\":\"record\",\"fields\":[{\"name\":\"raw_image_data\",\"type\":[\"bytes\",\"null\"]},{\"name\":\"rectangle_left\",\"type\":[\"int\",\"null\"]},{\"name\":\"rectangle_top\",\"type\":[\"int\",\"null\"]},{\"name\":\"rectangle_height\",\"type\":[\"int\",\"null\"]},{\"name\":\"rectangle_width\",\"type\":[\"int\",\"null\"]},{\"name\":\"face_id\",\"type\":[\"string\",\"null\"]},{\"name\":\"age\",\"type\":[\"double\",\"null\"]},{\"name\":\"mustache\",\"type\":[\"double\",\"null\"]},{\"name\":\"beard\",\"type\":[\"double\",\"null\"]},{\"name\":\"sideburns\",\"type\":[\"double\",\"null\"]},{\"name\":\"gender\",\"type\":[\"string\",\"null\"]},{\"name\":\"glasses\",\"type\":[\"string\",\"null\"]},{\"name\":\"head_pose_roll\",\"type\":[\"double\",\"null\"]},{\"name\":\"head_pose_yaw\",\"type\":[\"double\",\"null\"]},{\"name\":\"head_pose_pitch\",\"type\":[\"double\",\"null\"]},{\"name\":\"smile\",\"type\":[\"double\",\"null\"]},{\"name\":\"surprise\",\"type\":[\"double\",\"null\"]},{\"name\":\"neutral\",\"type\":[\"double\",\"null\"]},{\"name\":\"sadness\",\"type\":[\"double\",\"null\"]},{\"name\":\"anger\",\"type\":[\"double\",\"null\"]},{\"name\":\"contempt\",\"type\":[\"double\",\"null\"]},{\"name\":\"happiness\",\"type\":[\"double\",\"null\"]},{\"name\":\"disgust\",\"type\":[\"double\",\"null\"]},{\"name\":\"fear\",\"type\":[\"double\",\"null\"]}]}}]}",
    "doc.AzureFaceExtractor-transform": "# Azure Face Extractor Transform\n\n\nDescription\n-----------\nThis transform leverages the [CognitiveJ](https://github.com/CognitiveJ/cognitivej) library, and in turn the [Azure Cognitive APIs](https://azure.microsoft.com/en-us/services/cognitive-services/) to extract \nfaces and other metadata, including emotions, from a set of images. It is usually used in conjunction with the Whole File Reader plugin since it requires the entire contents of each image to be loaded into a single message and passed into the transform. \nDue to this, there may be memory issues when loading large images. This leverages the [Face API](https://docs.microsoft.com/en-us/azure/cognitive-services/face/) and the [Emotion API](https://docs.microsoft.com/en-us/azure/cognitive-services/emotion/home) specifically for this task. \n\nUse Case\n--------\nA developer is analyzing a large number of files and would like to identify faces in those photos to see how many men and women are in the photos. The user can combine this plugin with the whole file reader and filter using wrangler plugins based on the data extracted.\n\nProperties\n----------\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :------ | :---------- |\n| **Source Field Name** | **Y** | None | This is the name of the field on the input record containing the image file. It must be of type ``bytes`` and it must contain the entire contents of the image file. |\n| **Face API Key** | **Y** | None | This key is obtained from the Azure Portal after enabling the Face API. |\n| **Emotion API Key** | **Y** | None | This key is obtained from the Azure Portal after enabling the Emotion API. |\n| **Continue Processing If There Are Errors?** | **Y** | false | Indicates if the pipeline should continue if processing a single image fails. |\n\nUsage Notes\n-----------\n\nThis plugin requires an Azure account as well as having the Face and Emotion APIs enabled on the account. This plugin will make two calls per image, and pricing is based on requests to the service. Please see the [Pricing details](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/) for more information about pricing.\n\nThis plugin requires the entire contents of the image File to be loaded into memory for processing. This could cause issues when reading large images.\n\nFrom the Azure Docs:\n* The supported input image formats includes JPEG, PNG, GIF(the first frame), BMP. Image file size should be no larger than 4MB. \n* If a user has already called the Face API, they can submit the face rectangles as an optional input. Otherwise, Emotion API will first compute the rectangles. \n* The detectable face size range is 36x36 to 4096x4096 pixels. Faces out of this range will not be detected. \n* For each image, the maximum number of faces detected is 64 and the faces are ranked by face rectangle size in descending order. If no face is detected, an empty array will be returned. \n* Some faces may not be detected due to technical challenges, e.g. very large face angles (head-pose), large occlusion. Frontal and near-frontal faces have the best results. \n* The emotions contempt and disgust are experimental.\n  \n"
  }
}